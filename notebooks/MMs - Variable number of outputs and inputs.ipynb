{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import functools\n",
    "from dataset.conclusion_generation import test_for_mental_models, generate_random_tree, generate_and_save_trees\n",
    "from dataset.logic_tree import OperatorNode\n",
    "from dataset.encoding import encode_mental_models_separated_sentences, load_sentences_and_conclusions\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as kr\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.common import get_separated_sequences_mental_models_dataset\n",
    "import dataset.encoding\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.getcwd() + '\\\\temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 100 (0.2%) trees, correct: 92, recently correct: 92.0%, eta: 07s\n",
      "Checked 200 (0.4%) trees, correct: 179, recently correct: 87.0%, eta: 06s\n",
      "Checked 300 (0.6%) trees, correct: 253, recently correct: 74.0%, eta: 06s\n",
      "Checked 400 (0.8%) trees, correct: 330, recently correct: 77.0%, eta: 06s\n",
      "Checked 500 (1.0%) trees, correct: 403, recently correct: 73.0%, eta: 05s\n",
      "Checked 600 (1.2%) trees, correct: 466, recently correct: 63.0%, eta: 05s\n",
      "Checked 700 (1.4%) trees, correct: 531, recently correct: 65.0%, eta: 05s\n",
      "Checked 800 (1.6%) trees, correct: 598, recently correct: 67.0%, eta: 05s\n",
      "Checked 900 (1.8%) trees, correct: 664, recently correct: 66.0%, eta: 05s\n",
      "Checked 1000 (2.0%) trees, correct: 723, recently correct: 59.0%, eta: 05s\n",
      "Checked 1100 (2.2%) trees, correct: 784, recently correct: 61.0%, eta: 05s\n",
      "Checked 1200 (2.4%) trees, correct: 836, recently correct: 52.0%, eta: 05s\n",
      "Checked 1300 (2.6%) trees, correct: 884, recently correct: 48.0%, eta: 05s\n",
      "Checked 1400 (2.8%) trees, correct: 941, recently correct: 57.0%, eta: 05s\n",
      "Checked 1500 (3.0%) trees, correct: 990, recently correct: 49.0%, eta: 05s\n",
      "Checked 1600 (3.2%) trees, correct: 1032, recently correct: 42.0%, eta: 05s\n",
      "Checked 1700 (3.4%) trees, correct: 1084, recently correct: 52.0%, eta: 05s\n",
      "Checked 1800 (3.6%) trees, correct: 1130, recently correct: 46.0%, eta: 05s\n",
      "Checked 1900 (3.8%) trees, correct: 1170, recently correct: 40.0%, eta: 05s\n",
      "Checked 2000 (4.0%) trees, correct: 1210, recently correct: 40.0%, eta: 05s\n",
      "Checked 2100 (4.2%) trees, correct: 1254, recently correct: 44.0%, eta: 05s\n",
      "Checked 2200 (4.4%) trees, correct: 1291, recently correct: 37.0%, eta: 05s\n",
      "Checked 2300 (4.6%) trees, correct: 1334, recently correct: 43.0%, eta: 04s\n",
      "Checked 2400 (4.8%) trees, correct: 1369, recently correct: 35.0%, eta: 04s\n",
      "Checked 2500 (5.0%) trees, correct: 1404, recently correct: 35.0%, eta: 04s\n",
      "Checked 2600 (5.2%) trees, correct: 1436, recently correct: 32.0%, eta: 04s\n",
      "Checked 2700 (5.4%) trees, correct: 1468, recently correct: 32.0%, eta: 04s\n",
      "Checked 2800 (5.6%) trees, correct: 1505, recently correct: 37.0%, eta: 04s\n",
      "Checked 2900 (5.8%) trees, correct: 1530, recently correct: 25.0%, eta: 04s\n",
      "Checked 3000 (6.0%) trees, correct: 1563, recently correct: 33.0%, eta: 04s\n",
      "Checked 3100 (6.2%) trees, correct: 1592, recently correct: 29.0%, eta: 04s\n",
      "Checked 3200 (6.4%) trees, correct: 1632, recently correct: 40.0%, eta: 04s\n",
      "Checked 3300 (6.6%) trees, correct: 1657, recently correct: 25.0%, eta: 04s\n",
      "Checked 3400 (6.8%) trees, correct: 1685, recently correct: 28.0%, eta: 04s\n",
      "Checked 3500 (7.0%) trees, correct: 1712, recently correct: 27.0%, eta: 04s\n",
      "Checked 3600 (7.2%) trees, correct: 1746, recently correct: 34.0%, eta: 04s\n",
      "Checked 3700 (7.4%) trees, correct: 1775, recently correct: 29.0%, eta: 04s\n",
      "Checked 3800 (7.6%) trees, correct: 1807, recently correct: 32.0%, eta: 04s\n",
      "Checked 3900 (7.8%) trees, correct: 1830, recently correct: 23.0%, eta: 04s\n",
      "Checked 4000 (8.0%) trees, correct: 1858, recently correct: 28.0%, eta: 04s\n",
      "Checked 4100 (8.2%) trees, correct: 1883, recently correct: 25.0%, eta: 04s\n",
      "Checked 4200 (8.4%) trees, correct: 1908, recently correct: 25.0%, eta: 04s\n",
      "Checked 4300 (8.6%) trees, correct: 1933, recently correct: 25.0%, eta: 04s\n",
      "Checked 4400 (8.8%) trees, correct: 1963, recently correct: 30.0%, eta: 04s\n",
      "Checked 4500 (9.0%) trees, correct: 1982, recently correct: 19.0%, eta: 04s\n",
      "Checked 4600 (9.2%) trees, correct: 2006, recently correct: 24.0%, eta: 04s\n",
      "Checked 4700 (9.4%) trees, correct: 2031, recently correct: 25.0%, eta: 04s\n",
      "Checked 4800 (9.6%) trees, correct: 2058, recently correct: 27.0%, eta: 04s\n",
      "Checked 4900 (9.8%) trees, correct: 2072, recently correct: 14.0%, eta: 04s\n",
      "Checked 5000 (10.0%) trees, correct: 2093, recently correct: 21.0%, eta: 04s\n",
      "Checked 5100 (10.2%) trees, correct: 2122, recently correct: 29.0%, eta: 04s\n",
      "Checked 5200 (10.4%) trees, correct: 2140, recently correct: 18.0%, eta: 04s\n",
      "Checked 5300 (10.6%) trees, correct: 2160, recently correct: 20.0%, eta: 04s\n",
      "Checked 5400 (10.8%) trees, correct: 2184, recently correct: 24.0%, eta: 04s\n",
      "Checked 5500 (11.0%) trees, correct: 2205, recently correct: 21.0%, eta: 04s\n",
      "Checked 5600 (11.2%) trees, correct: 2225, recently correct: 20.0%, eta: 04s\n",
      "Checked 5700 (11.4%) trees, correct: 2243, recently correct: 18.0%, eta: 04s\n",
      "Checked 5800 (11.6%) trees, correct: 2256, recently correct: 13.0%, eta: 04s\n",
      "Checked 5900 (11.8%) trees, correct: 2277, recently correct: 21.0%, eta: 04s\n",
      "Checked 6000 (12.0%) trees, correct: 2294, recently correct: 17.0%, eta: 04s\n",
      "Checked 6100 (12.2%) trees, correct: 2314, recently correct: 20.0%, eta: 04s\n",
      "Checked 6200 (12.4%) trees, correct: 2333, recently correct: 19.0%, eta: 04s\n",
      "Checked 6300 (12.6%) trees, correct: 2344, recently correct: 11.0%, eta: 04s\n",
      "Checked 6400 (12.8%) trees, correct: 2361, recently correct: 17.0%, eta: 04s\n",
      "Checked 6500 (13.0%) trees, correct: 2376, recently correct: 15.0%, eta: 04s\n",
      "Checked 6600 (13.2%) trees, correct: 2395, recently correct: 19.0%, eta: 04s\n",
      "Checked 6700 (13.4%) trees, correct: 2412, recently correct: 17.0%, eta: 04s\n",
      "Checked 6800 (13.6%) trees, correct: 2432, recently correct: 20.0%, eta: 04s\n",
      "Checked 6900 (13.8%) trees, correct: 2449, recently correct: 17.0%, eta: 04s\n",
      "Checked 7000 (14.0%) trees, correct: 2459, recently correct: 10.0%, eta: 04s\n",
      "Checked 7100 (14.2%) trees, correct: 2476, recently correct: 17.0%, eta: 04s\n",
      "Checked 7200 (14.4%) trees, correct: 2491, recently correct: 15.0%, eta: 04s\n",
      "Checked 7300 (14.6%) trees, correct: 2505, recently correct: 14.0%, eta: 04s\n",
      "Checked 7400 (14.8%) trees, correct: 2522, recently correct: 17.0%, eta: 04s\n",
      "Checked 7500 (15.0%) trees, correct: 2537, recently correct: 15.0%, eta: 04s\n",
      "Checked 7600 (15.2%) trees, correct: 2547, recently correct: 10.0%, eta: 04s\n",
      "Checked 7700 (15.4%) trees, correct: 2564, recently correct: 17.0%, eta: 04s\n",
      "Checked 7800 (15.6%) trees, correct: 2577, recently correct: 13.0%, eta: 04s\n",
      "Checked 7900 (15.8%) trees, correct: 2593, recently correct: 16.0%, eta: 04s\n",
      "Checked 8000 (16.0%) trees, correct: 2599, recently correct: 6.0%, eta: 04s\n",
      "Checked 8100 (16.2%) trees, correct: 2612, recently correct: 13.0%, eta: 04s\n",
      "Checked 8200 (16.4%) trees, correct: 2626, recently correct: 14.0%, eta: 04s\n",
      "Checked 8300 (16.6%) trees, correct: 2643, recently correct: 17.0%, eta: 04s\n",
      "Checked 8400 (16.8%) trees, correct: 2655, recently correct: 12.0%, eta: 04s\n",
      "Checked 8500 (17.0%) trees, correct: 2665, recently correct: 10.0%, eta: 04s\n",
      "Checked 8600 (17.2%) trees, correct: 2678, recently correct: 13.0%, eta: 04s\n",
      "Checked 8700 (17.4%) trees, correct: 2695, recently correct: 17.0%, eta: 04s\n",
      "Checked 8800 (17.6%) trees, correct: 2713, recently correct: 18.0%, eta: 04s\n",
      "Checked 8900 (17.8%) trees, correct: 2724, recently correct: 11.0%, eta: 04s\n",
      "Checked 9000 (18.0%) trees, correct: 2734, recently correct: 10.0%, eta: 04s\n",
      "Checked 9100 (18.2%) trees, correct: 2742, recently correct: 8.0%, eta: 04s\n",
      "Checked 9200 (18.4%) trees, correct: 2751, recently correct: 9.0%, eta: 04s\n",
      "Checked 9300 (18.6%) trees, correct: 2762, recently correct: 11.0%, eta: 04s\n",
      "Checked 9400 (18.8%) trees, correct: 2769, recently correct: 7.0%, eta: 04s\n",
      "Checked 9500 (19.0%) trees, correct: 2777, recently correct: 8.0%, eta: 04s\n",
      "Checked 9600 (19.2%) trees, correct: 2785, recently correct: 8.0%, eta: 04s\n",
      "Checked 9700 (19.4%) trees, correct: 2800, recently correct: 15.0%, eta: 04s\n",
      "Checked 9800 (19.6%) trees, correct: 2811, recently correct: 11.0%, eta: 04s\n",
      "Checked 9900 (19.8%) trees, correct: 2819, recently correct: 8.0%, eta: 04s\n",
      "Checked 10000 (20.0%) trees, correct: 2834, recently correct: 15.0%, eta: 04s\n",
      "Checked 10100 (20.2%) trees, correct: 2840, recently correct: 6.0%, eta: 04s\n",
      "Checked 10200 (20.4%) trees, correct: 2848, recently correct: 8.0%, eta: 04s\n",
      "Checked 10300 (20.6%) trees, correct: 2858, recently correct: 10.0%, eta: 04s\n",
      "Checked 10400 (20.8%) trees, correct: 2867, recently correct: 9.0%, eta: 04s\n",
      "Checked 10500 (21.0%) trees, correct: 2877, recently correct: 10.0%, eta: 04s\n",
      "Checked 10600 (21.2%) trees, correct: 2891, recently correct: 14.0%, eta: 04s\n",
      "Checked 10700 (21.4%) trees, correct: 2901, recently correct: 10.0%, eta: 04s\n",
      "Checked 10800 (21.6%) trees, correct: 2909, recently correct: 8.0%, eta: 04s\n",
      "Checked 10900 (21.8%) trees, correct: 2916, recently correct: 7.0%, eta: 04s\n",
      "Checked 11000 (22.0%) trees, correct: 2924, recently correct: 8.0%, eta: 04s\n",
      "Checked 11100 (22.2%) trees, correct: 2931, recently correct: 7.0%, eta: 04s\n",
      "Checked 11200 (22.4%) trees, correct: 2941, recently correct: 10.0%, eta: 04s\n",
      "Checked 11300 (22.6%) trees, correct: 2948, recently correct: 7.0%, eta: 04s\n",
      "Checked 11400 (22.8%) trees, correct: 2955, recently correct: 7.0%, eta: 04s\n",
      "Checked 11500 (23.0%) trees, correct: 2960, recently correct: 5.0%, eta: 04s\n",
      "Checked 11600 (23.2%) trees, correct: 2967, recently correct: 7.0%, eta: 04s\n",
      "Checked 11700 (23.4%) trees, correct: 2980, recently correct: 13.0%, eta: 04s\n",
      "Checked 11800 (23.6%) trees, correct: 2989, recently correct: 9.0%, eta: 04s\n",
      "Checked 11900 (23.8%) trees, correct: 2999, recently correct: 10.0%, eta: 04s\n",
      "Checked 12000 (24.0%) trees, correct: 3006, recently correct: 7.0%, eta: 04s\n",
      "Checked 12100 (24.2%) trees, correct: 3009, recently correct: 3.0%, eta: 04s\n",
      "Checked 12200 (24.4%) trees, correct: 3017, recently correct: 8.0%, eta: 04s\n",
      "Checked 12300 (24.6%) trees, correct: 3022, recently correct: 5.0%, eta: 04s\n",
      "Checked 12400 (24.8%) trees, correct: 3028, recently correct: 6.0%, eta: 04s\n",
      "Checked 12500 (25.0%) trees, correct: 3035, recently correct: 7.0%, eta: 04s\n",
      "Checked 12600 (25.2%) trees, correct: 3040, recently correct: 5.0%, eta: 04s\n",
      "Checked 12700 (25.4%) trees, correct: 3048, recently correct: 8.0%, eta: 04s\n",
      "Checked 12800 (25.6%) trees, correct: 3054, recently correct: 6.0%, eta: 04s\n",
      "Checked 12900 (25.8%) trees, correct: 3062, recently correct: 8.0%, eta: 04s\n",
      "Checked 13000 (26.0%) trees, correct: 3065, recently correct: 3.0%, eta: 04s\n",
      "Checked 13100 (26.2%) trees, correct: 3071, recently correct: 6.0%, eta: 04s\n",
      "Checked 13200 (26.4%) trees, correct: 3076, recently correct: 5.0%, eta: 04s\n",
      "Checked 13300 (26.6%) trees, correct: 3080, recently correct: 4.0%, eta: 04s\n",
      "Checked 13400 (26.8%) trees, correct: 3090, recently correct: 10.0%, eta: 04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 13500 (27.0%) trees, correct: 3092, recently correct: 2.0%, eta: 04s\n",
      "Checked 13600 (27.2%) trees, correct: 3103, recently correct: 11.0%, eta: 04s\n",
      "Checked 13700 (27.4%) trees, correct: 3110, recently correct: 7.0%, eta: 04s\n",
      "Checked 13800 (27.6%) trees, correct: 3118, recently correct: 8.0%, eta: 04s\n",
      "Checked 13900 (27.8%) trees, correct: 3121, recently correct: 3.0%, eta: 04s\n",
      "Checked 14000 (28.0%) trees, correct: 3125, recently correct: 4.0%, eta: 04s\n",
      "Checked 14100 (28.2%) trees, correct: 3131, recently correct: 6.0%, eta: 04s\n",
      "Checked 14200 (28.4%) trees, correct: 3137, recently correct: 6.0%, eta: 04s\n",
      "Checked 14300 (28.6%) trees, correct: 3140, recently correct: 3.0%, eta: 04s\n",
      "Checked 14400 (28.8%) trees, correct: 3148, recently correct: 8.0%, eta: 04s\n",
      "Checked 14500 (29.0%) trees, correct: 3150, recently correct: 2.0%, eta: 04s\n",
      "Checked 14600 (29.2%) trees, correct: 3155, recently correct: 5.0%, eta: 04s\n",
      "Checked 14700 (29.4%) trees, correct: 3160, recently correct: 5.0%, eta: 04s\n",
      "Checked 14800 (29.6%) trees, correct: 3164, recently correct: 4.0%, eta: 04s\n",
      "Checked 14900 (29.8%) trees, correct: 3168, recently correct: 4.0%, eta: 04s\n",
      "Checked 15000 (30.0%) trees, correct: 3172, recently correct: 4.0%, eta: 03s\n",
      "Checked 15100 (30.2%) trees, correct: 3177, recently correct: 5.0%, eta: 03s\n",
      "Checked 15200 (30.4%) trees, correct: 3182, recently correct: 5.0%, eta: 03s\n",
      "Checked 15300 (30.6%) trees, correct: 3187, recently correct: 5.0%, eta: 03s\n",
      "Checked 15400 (30.8%) trees, correct: 3192, recently correct: 5.0%, eta: 03s\n",
      "Checked 15500 (31.0%) trees, correct: 3200, recently correct: 8.0%, eta: 03s\n",
      "Checked 15600 (31.2%) trees, correct: 3205, recently correct: 5.0%, eta: 03s\n",
      "Checked 15700 (31.4%) trees, correct: 3208, recently correct: 3.0%, eta: 03s\n",
      "Checked 15800 (31.6%) trees, correct: 3212, recently correct: 4.0%, eta: 03s\n",
      "Checked 15900 (31.8%) trees, correct: 3216, recently correct: 4.0%, eta: 03s\n",
      "Checked 16000 (32.0%) trees, correct: 3220, recently correct: 4.0%, eta: 03s\n",
      "Checked 16100 (32.2%) trees, correct: 3224, recently correct: 4.0%, eta: 03s\n",
      "Checked 16200 (32.4%) trees, correct: 3225, recently correct: 1.0%, eta: 03s\n",
      "Checked 16300 (32.6%) trees, correct: 3231, recently correct: 6.0%, eta: 03s\n",
      "Checked 16400 (32.8%) trees, correct: 3235, recently correct: 4.0%, eta: 03s\n",
      "Checked 16500 (33.0%) trees, correct: 3237, recently correct: 2.0%, eta: 03s\n",
      "Checked 16600 (33.2%) trees, correct: 3243, recently correct: 6.0%, eta: 03s\n",
      "Checked 16700 (33.4%) trees, correct: 3247, recently correct: 4.0%, eta: 03s\n",
      "Checked 16800 (33.6%) trees, correct: 3250, recently correct: 3.0%, eta: 03s\n",
      "Checked 16900 (33.8%) trees, correct: 3254, recently correct: 4.0%, eta: 03s\n",
      "Checked 17000 (34.0%) trees, correct: 3259, recently correct: 5.0%, eta: 03s\n",
      "Checked 17100 (34.2%) trees, correct: 3261, recently correct: 2.0%, eta: 03s\n",
      "Checked 17200 (34.4%) trees, correct: 3267, recently correct: 6.0%, eta: 03s\n",
      "Checked 17300 (34.6%) trees, correct: 3270, recently correct: 3.0%, eta: 03s\n",
      "Checked 17400 (34.8%) trees, correct: 3273, recently correct: 3.0%, eta: 03s\n",
      "Checked 17500 (35.0%) trees, correct: 3276, recently correct: 3.0%, eta: 03s\n",
      "Checked 17600 (35.2%) trees, correct: 3278, recently correct: 2.0%, eta: 03s\n",
      "Checked 17700 (35.4%) trees, correct: 3282, recently correct: 4.0%, eta: 03s\n",
      "Checked 17800 (35.6%) trees, correct: 3289, recently correct: 7.0%, eta: 03s\n",
      "Checked 17900 (35.8%) trees, correct: 3292, recently correct: 3.0%, eta: 03s\n",
      "Checked 18000 (36.0%) trees, correct: 3295, recently correct: 3.0%, eta: 03s\n",
      "Checked 18100 (36.2%) trees, correct: 3297, recently correct: 2.0%, eta: 03s\n",
      "Checked 18200 (36.4%) trees, correct: 3297, recently correct: 0.0%, eta: 03s\n",
      "Checked 18300 (36.6%) trees, correct: 3298, recently correct: 1.0%, eta: 03s\n",
      "Checked 18400 (36.8%) trees, correct: 3301, recently correct: 3.0%, eta: 03s\n",
      "Checked 18500 (37.0%) trees, correct: 3301, recently correct: 0.0%, eta: 03s\n",
      "Checked 18600 (37.2%) trees, correct: 3301, recently correct: 0.0%, eta: 03s\n",
      "Checked 18700 (37.4%) trees, correct: 3304, recently correct: 3.0%, eta: 03s\n",
      "Checked 18800 (37.6%) trees, correct: 3309, recently correct: 5.0%, eta: 03s\n",
      "Checked 18900 (37.8%) trees, correct: 3311, recently correct: 2.0%, eta: 03s\n",
      "Checked 19000 (38.0%) trees, correct: 3311, recently correct: 0.0%, eta: 03s\n",
      "Checked 19100 (38.2%) trees, correct: 3315, recently correct: 4.0%, eta: 03s\n",
      "Checked 19200 (38.4%) trees, correct: 3317, recently correct: 2.0%, eta: 03s\n",
      "Checked 19300 (38.6%) trees, correct: 3318, recently correct: 1.0%, eta: 03s\n",
      "Checked 19400 (38.8%) trees, correct: 3319, recently correct: 1.0%, eta: 03s\n",
      "Checked 19500 (39.0%) trees, correct: 3320, recently correct: 1.0%, eta: 03s\n",
      "Checked 19600 (39.2%) trees, correct: 3325, recently correct: 5.0%, eta: 03s\n",
      "Checked 19700 (39.4%) trees, correct: 3326, recently correct: 1.0%, eta: 03s\n",
      "Checked 19800 (39.6%) trees, correct: 3329, recently correct: 3.0%, eta: 03s\n",
      "Checked 19900 (39.8%) trees, correct: 3332, recently correct: 3.0%, eta: 03s\n",
      "Checked 20000 (40.0%) trees, correct: 3334, recently correct: 2.0%, eta: 03s\n",
      "Checked 20100 (40.2%) trees, correct: 3335, recently correct: 1.0%, eta: 03s\n",
      "Checked 20200 (40.4%) trees, correct: 3337, recently correct: 2.0%, eta: 03s\n",
      "Checked 20300 (40.6%) trees, correct: 3339, recently correct: 2.0%, eta: 03s\n",
      "Checked 20400 (40.8%) trees, correct: 3343, recently correct: 4.0%, eta: 03s\n",
      "Checked 20500 (41.0%) trees, correct: 3347, recently correct: 4.0%, eta: 03s\n",
      "Checked 20600 (41.2%) trees, correct: 3351, recently correct: 4.0%, eta: 03s\n",
      "Checked 20700 (41.4%) trees, correct: 3353, recently correct: 2.0%, eta: 03s\n",
      "Checked 20800 (41.6%) trees, correct: 3353, recently correct: 0.0%, eta: 03s\n",
      "Checked 20900 (41.8%) trees, correct: 3355, recently correct: 2.0%, eta: 03s\n",
      "Checked 21000 (42.0%) trees, correct: 3359, recently correct: 4.0%, eta: 03s\n",
      "Checked 21100 (42.2%) trees, correct: 3361, recently correct: 2.0%, eta: 03s\n",
      "Checked 21200 (42.4%) trees, correct: 3361, recently correct: 0.0%, eta: 03s\n",
      "Checked 21300 (42.6%) trees, correct: 3364, recently correct: 3.0%, eta: 03s\n",
      "Checked 21400 (42.8%) trees, correct: 3364, recently correct: 0.0%, eta: 03s\n",
      "Checked 21500 (43.0%) trees, correct: 3366, recently correct: 2.0%, eta: 03s\n",
      "Checked 21600 (43.2%) trees, correct: 3367, recently correct: 1.0%, eta: 03s\n",
      "Checked 21700 (43.4%) trees, correct: 3371, recently correct: 4.0%, eta: 03s\n",
      "Checked 21800 (43.6%) trees, correct: 3374, recently correct: 3.0%, eta: 03s\n",
      "Checked 21900 (43.8%) trees, correct: 3375, recently correct: 1.0%, eta: 03s\n",
      "Checked 22000 (44.0%) trees, correct: 3375, recently correct: 0.0%, eta: 03s\n",
      "Checked 22100 (44.2%) trees, correct: 3378, recently correct: 3.0%, eta: 03s\n",
      "Checked 22200 (44.4%) trees, correct: 3378, recently correct: 0.0%, eta: 03s\n",
      "Checked 22300 (44.6%) trees, correct: 3378, recently correct: 0.0%, eta: 03s\n",
      "Checked 22400 (44.8%) trees, correct: 3378, recently correct: 0.0%, eta: 03s\n",
      "Checked 22500 (45.0%) trees, correct: 3382, recently correct: 4.0%, eta: 03s\n",
      "Checked 22600 (45.2%) trees, correct: 3387, recently correct: 5.0%, eta: 03s\n",
      "Checked 22700 (45.4%) trees, correct: 3389, recently correct: 2.0%, eta: 03s\n",
      "Checked 22800 (45.6%) trees, correct: 3392, recently correct: 3.0%, eta: 03s\n",
      "Checked 22900 (45.8%) trees, correct: 3394, recently correct: 2.0%, eta: 03s\n",
      "Checked 23000 (46.0%) trees, correct: 3395, recently correct: 1.0%, eta: 03s\n",
      "Checked 23100 (46.2%) trees, correct: 3395, recently correct: 0.0%, eta: 03s\n",
      "Checked 23200 (46.4%) trees, correct: 3396, recently correct: 1.0%, eta: 03s\n",
      "Checked 23300 (46.6%) trees, correct: 3397, recently correct: 1.0%, eta: 03s\n",
      "Checked 23400 (46.8%) trees, correct: 3397, recently correct: 0.0%, eta: 03s\n",
      "Checked 23500 (47.0%) trees, correct: 3399, recently correct: 2.0%, eta: 03s\n",
      "Checked 23600 (47.2%) trees, correct: 3400, recently correct: 1.0%, eta: 03s\n",
      "Checked 23700 (47.4%) trees, correct: 3401, recently correct: 1.0%, eta: 03s\n",
      "Checked 23800 (47.6%) trees, correct: 3401, recently correct: 0.0%, eta: 03s\n",
      "Checked 23900 (47.8%) trees, correct: 3402, recently correct: 1.0%, eta: 03s\n",
      "Checked 24000 (48.0%) trees, correct: 3405, recently correct: 3.0%, eta: 03s\n",
      "Checked 24100 (48.2%) trees, correct: 3407, recently correct: 2.0%, eta: 03s\n",
      "Checked 24200 (48.4%) trees, correct: 3407, recently correct: 0.0%, eta: 03s\n",
      "Checked 24300 (48.6%) trees, correct: 3407, recently correct: 0.0%, eta: 03s\n",
      "Checked 24400 (48.8%) trees, correct: 3409, recently correct: 2.0%, eta: 03s\n",
      "Checked 24500 (49.0%) trees, correct: 3410, recently correct: 1.0%, eta: 03s\n",
      "Checked 24600 (49.2%) trees, correct: 3412, recently correct: 2.0%, eta: 03s\n",
      "Checked 24700 (49.4%) trees, correct: 3413, recently correct: 1.0%, eta: 03s\n",
      "Checked 24800 (49.6%) trees, correct: 3416, recently correct: 3.0%, eta: 03s\n",
      "Checked 24900 (49.8%) trees, correct: 3417, recently correct: 1.0%, eta: 03s\n",
      "Checked 25000 (50.0%) trees, correct: 3418, recently correct: 1.0%, eta: 03s\n",
      "Checked 25100 (50.2%) trees, correct: 3420, recently correct: 2.0%, eta: 03s\n",
      "Checked 25200 (50.4%) trees, correct: 3420, recently correct: 0.0%, eta: 03s\n",
      "Checked 25300 (50.6%) trees, correct: 3422, recently correct: 2.0%, eta: 03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 25400 (50.8%) trees, correct: 3423, recently correct: 1.0%, eta: 03s\n",
      "Checked 25500 (51.0%) trees, correct: 3425, recently correct: 2.0%, eta: 03s\n",
      "Checked 25600 (51.2%) trees, correct: 3426, recently correct: 1.0%, eta: 03s\n",
      "Checked 25700 (51.4%) trees, correct: 3428, recently correct: 2.0%, eta: 03s\n",
      "Checked 25800 (51.6%) trees, correct: 3428, recently correct: 0.0%, eta: 02s\n",
      "Checked 25900 (51.8%) trees, correct: 3429, recently correct: 1.0%, eta: 02s\n",
      "Checked 26000 (52.0%) trees, correct: 3433, recently correct: 4.0%, eta: 02s\n",
      "Checked 26100 (52.2%) trees, correct: 3434, recently correct: 1.0%, eta: 02s\n",
      "Checked 26200 (52.4%) trees, correct: 3435, recently correct: 1.0%, eta: 02s\n",
      "Checked 26300 (52.6%) trees, correct: 3435, recently correct: 0.0%, eta: 02s\n",
      "Checked 26400 (52.8%) trees, correct: 3436, recently correct: 1.0%, eta: 02s\n",
      "Checked 26500 (53.0%) trees, correct: 3439, recently correct: 3.0%, eta: 02s\n",
      "Checked 26600 (53.2%) trees, correct: 3440, recently correct: 1.0%, eta: 02s\n",
      "Checked 26700 (53.4%) trees, correct: 3443, recently correct: 3.0%, eta: 02s\n",
      "Checked 26800 (53.6%) trees, correct: 3443, recently correct: 0.0%, eta: 02s\n",
      "Checked 26900 (53.8%) trees, correct: 3443, recently correct: 0.0%, eta: 02s\n",
      "Checked 27000 (54.0%) trees, correct: 3443, recently correct: 0.0%, eta: 02s\n",
      "Checked 27100 (54.2%) trees, correct: 3443, recently correct: 0.0%, eta: 02s\n",
      "Checked 27200 (54.4%) trees, correct: 3444, recently correct: 1.0%, eta: 02s\n",
      "Checked 27300 (54.6%) trees, correct: 3444, recently correct: 0.0%, eta: 02s\n",
      "Checked 27400 (54.8%) trees, correct: 3444, recently correct: 0.0%, eta: 02s\n",
      "Checked 27500 (55.0%) trees, correct: 3445, recently correct: 1.0%, eta: 02s\n",
      "Checked 27600 (55.2%) trees, correct: 3445, recently correct: 0.0%, eta: 02s\n",
      "Checked 27700 (55.4%) trees, correct: 3446, recently correct: 1.0%, eta: 02s\n",
      "Checked 27800 (55.6%) trees, correct: 3446, recently correct: 0.0%, eta: 02s\n",
      "Checked 27900 (55.8%) trees, correct: 3446, recently correct: 0.0%, eta: 02s\n",
      "Checked 28000 (56.0%) trees, correct: 3446, recently correct: 0.0%, eta: 02s\n",
      "Checked 28100 (56.2%) trees, correct: 3447, recently correct: 1.0%, eta: 02s\n",
      "Checked 28200 (56.4%) trees, correct: 3448, recently correct: 1.0%, eta: 02s\n",
      "Checked 28300 (56.6%) trees, correct: 3449, recently correct: 1.0%, eta: 02s\n",
      "Checked 28400 (56.8%) trees, correct: 3449, recently correct: 0.0%, eta: 02s\n",
      "Checked 28500 (57.0%) trees, correct: 3450, recently correct: 1.0%, eta: 02s\n",
      "Checked 28600 (57.2%) trees, correct: 3452, recently correct: 2.0%, eta: 02s\n",
      "Checked 28700 (57.4%) trees, correct: 3453, recently correct: 1.0%, eta: 02s\n",
      "Checked 28800 (57.6%) trees, correct: 3453, recently correct: 0.0%, eta: 02s\n",
      "Checked 28900 (57.8%) trees, correct: 3453, recently correct: 0.0%, eta: 02s\n",
      "Checked 29000 (58.0%) trees, correct: 3453, recently correct: 0.0%, eta: 02s\n",
      "Checked 29100 (58.2%) trees, correct: 3455, recently correct: 2.0%, eta: 02s\n",
      "Checked 29200 (58.4%) trees, correct: 3456, recently correct: 1.0%, eta: 02s\n",
      "Checked 29300 (58.6%) trees, correct: 3457, recently correct: 1.0%, eta: 02s\n",
      "Checked 29400 (58.8%) trees, correct: 3457, recently correct: 0.0%, eta: 02s\n",
      "Checked 29500 (59.0%) trees, correct: 3458, recently correct: 1.0%, eta: 02s\n",
      "Checked 29600 (59.2%) trees, correct: 3459, recently correct: 1.0%, eta: 02s\n",
      "Checked 29700 (59.4%) trees, correct: 3461, recently correct: 2.0%, eta: 02s\n",
      "Checked 29800 (59.6%) trees, correct: 3461, recently correct: 0.0%, eta: 02s\n",
      "Checked 29900 (59.8%) trees, correct: 3461, recently correct: 0.0%, eta: 02s\n",
      "Checked 30000 (60.0%) trees, correct: 3461, recently correct: 0.0%, eta: 02s\n",
      "Checked 30100 (60.2%) trees, correct: 3462, recently correct: 1.0%, eta: 02s\n",
      "Checked 30200 (60.4%) trees, correct: 3462, recently correct: 0.0%, eta: 02s\n",
      "Checked 30300 (60.6%) trees, correct: 3462, recently correct: 0.0%, eta: 02s\n",
      "Checked 30400 (60.8%) trees, correct: 3462, recently correct: 0.0%, eta: 02s\n",
      "Checked 30500 (61.0%) trees, correct: 3462, recently correct: 0.0%, eta: 02s\n",
      "Checked 30600 (61.2%) trees, correct: 3462, recently correct: 0.0%, eta: 02s\n",
      "Checked 30700 (61.4%) trees, correct: 3463, recently correct: 1.0%, eta: 02s\n",
      "Checked 30800 (61.6%) trees, correct: 3463, recently correct: 0.0%, eta: 02s\n",
      "Checked 30900 (61.8%) trees, correct: 3463, recently correct: 0.0%, eta: 02s\n",
      "Checked 31000 (62.0%) trees, correct: 3464, recently correct: 1.0%, eta: 02s\n",
      "Checked 31100 (62.2%) trees, correct: 3465, recently correct: 1.0%, eta: 02s\n",
      "Checked 31200 (62.4%) trees, correct: 3465, recently correct: 0.0%, eta: 02s\n",
      "Checked 31300 (62.6%) trees, correct: 3465, recently correct: 0.0%, eta: 02s\n",
      "Checked 31400 (62.8%) trees, correct: 3466, recently correct: 1.0%, eta: 02s\n",
      "Checked 31500 (63.0%) trees, correct: 3466, recently correct: 0.0%, eta: 02s\n",
      "Checked 31600 (63.2%) trees, correct: 3466, recently correct: 0.0%, eta: 02s\n",
      "Checked 31700 (63.4%) trees, correct: 3466, recently correct: 0.0%, eta: 02s\n",
      "Checked 31800 (63.6%) trees, correct: 3466, recently correct: 0.0%, eta: 02s\n",
      "Checked 31900 (63.8%) trees, correct: 3466, recently correct: 0.0%, eta: 02s\n",
      "Checked 32000 (64.0%) trees, correct: 3467, recently correct: 1.0%, eta: 02s\n",
      "Checked 32100 (64.2%) trees, correct: 3467, recently correct: 0.0%, eta: 02s\n",
      "Checked 32200 (64.4%) trees, correct: 3467, recently correct: 0.0%, eta: 02s\n",
      "Checked 32300 (64.6%) trees, correct: 3467, recently correct: 0.0%, eta: 02s\n",
      "Checked 32400 (64.8%) trees, correct: 3467, recently correct: 0.0%, eta: 02s\n",
      "Checked 32500 (65.0%) trees, correct: 3468, recently correct: 1.0%, eta: 02s\n",
      "Checked 32600 (65.2%) trees, correct: 3468, recently correct: 0.0%, eta: 02s\n",
      "Checked 32700 (65.4%) trees, correct: 3468, recently correct: 0.0%, eta: 02s\n",
      "Checked 32800 (65.6%) trees, correct: 3469, recently correct: 1.0%, eta: 02s\n",
      "Checked 32900 (65.8%) trees, correct: 3470, recently correct: 1.0%, eta: 02s\n",
      "Checked 33000 (66.0%) trees, correct: 3470, recently correct: 0.0%, eta: 02s\n",
      "Checked 33100 (66.2%) trees, correct: 3470, recently correct: 0.0%, eta: 02s\n",
      "Checked 33200 (66.4%) trees, correct: 3470, recently correct: 0.0%, eta: 02s\n",
      "Checked 33300 (66.6%) trees, correct: 3470, recently correct: 0.0%, eta: 02s\n",
      "Checked 33400 (66.8%) trees, correct: 3470, recently correct: 0.0%, eta: 02s\n",
      "Checked 33500 (67.0%) trees, correct: 3470, recently correct: 0.0%, eta: 02s\n",
      "Checked 33600 (67.2%) trees, correct: 3471, recently correct: 1.0%, eta: 02s\n",
      "Checked 33700 (67.4%) trees, correct: 3471, recently correct: 0.0%, eta: 02s\n",
      "Checked 33800 (67.6%) trees, correct: 3471, recently correct: 0.0%, eta: 02s\n",
      "Checked 33900 (67.8%) trees, correct: 3471, recently correct: 0.0%, eta: 02s\n",
      "Checked 34000 (68.0%) trees, correct: 3471, recently correct: 0.0%, eta: 01s\n",
      "Checked 34100 (68.2%) trees, correct: 3472, recently correct: 1.0%, eta: 01s\n",
      "Checked 34200 (68.4%) trees, correct: 3472, recently correct: 0.0%, eta: 01s\n",
      "Checked 34300 (68.6%) trees, correct: 3472, recently correct: 0.0%, eta: 01s\n",
      "Checked 34400 (68.8%) trees, correct: 3472, recently correct: 0.0%, eta: 01s\n",
      "Checked 34500 (69.0%) trees, correct: 3473, recently correct: 1.0%, eta: 01s\n",
      "Checked 34600 (69.2%) trees, correct: 3473, recently correct: 0.0%, eta: 01s\n",
      "Checked 34700 (69.4%) trees, correct: 3473, recently correct: 0.0%, eta: 01s\n",
      "Checked 34800 (69.6%) trees, correct: 3473, recently correct: 0.0%, eta: 01s\n",
      "Checked 34900 (69.8%) trees, correct: 3474, recently correct: 1.0%, eta: 01s\n",
      "Checked 35000 (70.0%) trees, correct: 3474, recently correct: 0.0%, eta: 01s\n",
      "Checked 35100 (70.2%) trees, correct: 3474, recently correct: 0.0%, eta: 01s\n",
      "Checked 35200 (70.4%) trees, correct: 3474, recently correct: 0.0%, eta: 01s\n",
      "Checked 35300 (70.6%) trees, correct: 3474, recently correct: 0.0%, eta: 01s\n",
      "Checked 35400 (70.8%) trees, correct: 3475, recently correct: 1.0%, eta: 01s\n",
      "Checked 35500 (71.0%) trees, correct: 3475, recently correct: 0.0%, eta: 01s\n",
      "Checked 35600 (71.2%) trees, correct: 3475, recently correct: 0.0%, eta: 01s\n",
      "Checked 35700 (71.4%) trees, correct: 3475, recently correct: 0.0%, eta: 01s\n",
      "Checked 35800 (71.6%) trees, correct: 3475, recently correct: 0.0%, eta: 01s\n",
      "Checked 35900 (71.8%) trees, correct: 3475, recently correct: 0.0%, eta: 01s\n",
      "Checked 36000 (72.0%) trees, correct: 3476, recently correct: 1.0%, eta: 01s\n",
      "Checked 36100 (72.2%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 36200 (72.4%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 36300 (72.6%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 36400 (72.8%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 36500 (73.0%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 36600 (73.2%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 36700 (73.4%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 36800 (73.6%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 36900 (73.8%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 37000 (74.0%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 37100 (74.2%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 37200 (74.4%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 37300 (74.6%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 37400 (74.8%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 37500 (75.0%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 37600 (75.2%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 37700 (75.4%) trees, correct: 3478, recently correct: 2.0%, eta: 01s\n",
      "Checked 37800 (75.6%) trees, correct: 3478, recently correct: 0.0%, eta: 01s\n",
      "Checked 37900 (75.8%) trees, correct: 3478, recently correct: 0.0%, eta: 01s\n",
      "Checked 38000 (76.0%) trees, correct: 3478, recently correct: 0.0%, eta: 01s\n",
      "Checked 38100 (76.2%) trees, correct: 3478, recently correct: 0.0%, eta: 01s\n",
      "Checked 38200 (76.4%) trees, correct: 3479, recently correct: 1.0%, eta: 01s\n",
      "Checked 38300 (76.6%) trees, correct: 3480, recently correct: 1.0%, eta: 01s\n",
      "Checked 38400 (76.8%) trees, correct: 3480, recently correct: 0.0%, eta: 01s\n",
      "Checked 38500 (77.0%) trees, correct: 3480, recently correct: 0.0%, eta: 01s\n",
      "Checked 38600 (77.2%) trees, correct: 3480, recently correct: 0.0%, eta: 01s\n",
      "Checked 38700 (77.4%) trees, correct: 3481, recently correct: 1.0%, eta: 01s\n",
      "Checked 38800 (77.6%) trees, correct: 3481, recently correct: 0.0%, eta: 01s\n",
      "Checked 38900 (77.8%) trees, correct: 3481, recently correct: 0.0%, eta: 01s\n",
      "Checked 39000 (78.0%) trees, correct: 3482, recently correct: 1.0%, eta: 01s\n",
      "Checked 39100 (78.2%) trees, correct: 3482, recently correct: 0.0%, eta: 01s\n",
      "Checked 39200 (78.4%) trees, correct: 3482, recently correct: 0.0%, eta: 01s\n",
      "Checked 39300 (78.6%) trees, correct: 3482, recently correct: 0.0%, eta: 01s\n",
      "Checked 39400 (78.8%) trees, correct: 3482, recently correct: 0.0%, eta: 01s\n",
      "Checked 39500 (79.0%) trees, correct: 3482, recently correct: 0.0%, eta: 01s\n",
      "Checked 39600 (79.2%) trees, correct: 3482, recently correct: 0.0%, eta: 01s\n",
      "Checked 39700 (79.4%) trees, correct: 3482, recently correct: 0.0%, eta: 01s\n",
      "Checked 39800 (79.6%) trees, correct: 3482, recently correct: 0.0%, eta: 01s\n",
      "Checked 39900 (79.8%) trees, correct: 3482, recently correct: 0.0%, eta: 01s\n",
      "Checked 40000 (80.0%) trees, correct: 3482, recently correct: 0.0%, eta: 01s\n",
      "Checked 40100 (80.2%) trees, correct: 3482, recently correct: 0.0%, eta: 01s\n",
      "Checked 40200 (80.4%) trees, correct: 3484, recently correct: 2.0%, eta: 01s\n",
      "Checked 40300 (80.6%) trees, correct: 3485, recently correct: 1.0%, eta: 01s\n",
      "Checked 40400 (80.8%) trees, correct: 3486, recently correct: 1.0%, eta: 01s\n",
      "Checked 40500 (81.0%) trees, correct: 3486, recently correct: 0.0%, eta: 01s\n",
      "Checked 40600 (81.2%) trees, correct: 3486, recently correct: 0.0%, eta: 01s\n",
      "Checked 40700 (81.4%) trees, correct: 3486, recently correct: 0.0%, eta: 01s\n",
      "Checked 40800 (81.6%) trees, correct: 3486, recently correct: 0.0%, eta: 01s\n",
      "Checked 40900 (81.8%) trees, correct: 3486, recently correct: 0.0%, eta: 01s\n",
      "Checked 41000 (82.0%) trees, correct: 3486, recently correct: 0.0%, eta: 01s\n",
      "Checked 41100 (82.2%) trees, correct: 3486, recently correct: 0.0%, eta: 01s\n",
      "Checked 41200 (82.4%) trees, correct: 3487, recently correct: 1.0%, eta: 01s\n",
      "Checked 41300 (82.6%) trees, correct: 3487, recently correct: 0.0%, eta: 01s\n",
      "Checked 41400 (82.8%) trees, correct: 3487, recently correct: 0.0%, eta: 01s\n",
      "Checked 41500 (83.0%) trees, correct: 3487, recently correct: 0.0%, eta: 01s\n",
      "Checked 41600 (83.2%) trees, correct: 3487, recently correct: 0.0%, eta: 01s\n",
      "Checked 41700 (83.4%) trees, correct: 3487, recently correct: 0.0%, eta: 01s\n",
      "Checked 41800 (83.6%) trees, correct: 3487, recently correct: 0.0%, eta: 01s\n",
      "Checked 41900 (83.8%) trees, correct: 3487, recently correct: 0.0%, eta: 01s\n",
      "Checked 42000 (84.0%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 42100 (84.2%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 42200 (84.4%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 42300 (84.6%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 42400 (84.8%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 42500 (85.0%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 42600 (85.2%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 42700 (85.4%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 42800 (85.6%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 42900 (85.8%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 43000 (86.0%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 43100 (86.2%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 43200 (86.4%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 43300 (86.6%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 43400 (86.8%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 43500 (87.0%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 43600 (87.2%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 43700 (87.4%) trees, correct: 3488, recently correct: 1.0%, eta: 00s\n",
      "Checked 43800 (87.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 43900 (87.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 44000 (88.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 44100 (88.2%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 44200 (88.4%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 44300 (88.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 44400 (88.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 44500 (89.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 44600 (89.2%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 44700 (89.4%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 44800 (89.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 44900 (89.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 45000 (90.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 45100 (90.2%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 45200 (90.4%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 45300 (90.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 45400 (90.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 45500 (91.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 45600 (91.2%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 45700 (91.4%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 45800 (91.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 45900 (91.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 46000 (92.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 46100 (92.2%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 46200 (92.4%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 46300 (92.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 46400 (92.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 46500 (93.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 46600 (93.2%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 46700 (93.4%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 46800 (93.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 46900 (93.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 47000 (94.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 47100 (94.2%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 47200 (94.4%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 47300 (94.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 47400 (94.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 47500 (95.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 47600 (95.2%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 47700 (95.4%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 47800 (95.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 47900 (95.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48000 (96.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48100 (96.2%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48200 (96.4%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48300 (96.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48400 (96.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48500 (97.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48600 (97.2%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48700 (97.4%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48800 (97.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48900 (97.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49000 (98.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49100 (98.2%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49200 (98.4%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49300 (98.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49400 (98.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49500 (99.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49600 (99.2%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49700 (99.4%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49800 (99.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49900 (99.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 50000 (100.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "found trees: 3488\n",
      "infix tree: not 3 sep 5\n",
      "prefix tree: sep not 3 5\n",
      "conclusion: nnFnT\n",
      "\n",
      "infix tree: ( 3 and 2 ) sep ( 2 and 4 )\n",
      "prefix tree: sep and 3 2 and 2 4\n",
      "conclusion: nTTTn\n",
      "\n",
      "infix tree: ( 2 or 2 ) sep ( 4 or 1 )\n",
      "prefix tree: sep or 2 2 or 4 1\n",
      "conclusion: nTnTn,TTnnn\n",
      "\n",
      "infix tree: 2 sep ( 2 or 3 )\n",
      "prefix tree: sep 2 or 2 3\n",
      "conclusion: nTnnn\n",
      "\n",
      "infix tree: ( 5 or 2 ) sep not 1\n",
      "prefix tree: sep or 5 2 not 1\n",
      "conclusion: FnnnT,FTnnn\n",
      "\n",
      "infix tree: not 4 sep ( 2 or 3 )\n",
      "prefix tree: sep not 4 or 2 3\n",
      "conclusion: nnTFn,nTnFn\n",
      "\n",
      "infix tree: ( 3 and 3 ) sep not 1\n",
      "prefix tree: sep and 3 3 not 1\n",
      "conclusion: FnTnn\n",
      "\n",
      "infix tree: 4 sep 4\n",
      "prefix tree: sep 4 4\n",
      "conclusion: nnnTn\n",
      "\n",
      "infix tree: not 3 sep ( 1 and 5 )\n",
      "prefix tree: sep not 3 and 1 5\n",
      "conclusion: TnFnT\n",
      "\n",
      "infix tree: ( 1 and 3 ) sep ( 5 and 5 )\n",
      "prefix tree: sep and 1 3 and 5 5\n",
      "conclusion: TnTnT\n",
      "\n",
      "infix tree: ( 5 and 3 ) sep not 2\n",
      "prefix tree: sep and 5 3 not 2\n",
      "conclusion: nFTnT\n",
      "\n",
      "infix tree: not 2 sep ( 5 or 1 )\n",
      "prefix tree: sep not 2 or 5 1\n",
      "conclusion: nFnnT,TFnnn\n",
      "\n",
      "infix tree: ( 3 or 1 ) sep ( 3 or 3 )\n",
      "prefix tree: sep or 3 1 or 3 3\n",
      "conclusion: nnTnn\n",
      "\n",
      "infix tree: not 5 sep ( 4 and 3 )\n",
      "prefix tree: sep not 5 and 4 3\n",
      "conclusion: nnTTF\n",
      "\n",
      "infix tree: ( 1 and 1 ) sep ( 4 and 4 )\n",
      "prefix tree: sep and 1 1 and 4 4\n",
      "conclusion: TnnTn\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])]\n",
      "[array([ 0.,  0., -1.,  0.,  1.])]\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])]\n",
      "[array([0., 1., 1., 1., 0.])]\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])]\n",
      "[array([0., 1., 0., 1., 0.]), array([1., 1., 0., 0., 0.])]\n",
      "\n",
      "[array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])]\n",
      "[array([0., 1., 0., 0., 0.])]\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])]\n",
      "[array([-1.,  0.,  0.,  0.,  1.]), array([-1.,  1.,  0.,  0.,  0.])]\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])]\n",
      "[array([ 0.,  0.,  1., -1.,  0.]), array([ 0.,  1.,  0., -1.,  0.])]\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])]\n",
      "[array([-1.,  0.,  1.,  0.,  0.])]\n",
      "\n",
      "[array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]])]\n",
      "[array([0., 0., 0., 1., 0.])]\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])]\n",
      "[array([ 1.,  0., -1.,  0.,  1.])]\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])]\n",
      "[array([1., 0., 1., 0., 1.])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# max_depth = 2\n",
    "# num_variables = 5\n",
    "# input_length = 10\n",
    "# output_length = 5\n",
    "\n",
    "# class SeparatorNode(OperatorNode):\n",
    "#     accepts_children = 2\n",
    "\n",
    "#     def __init__(self, *children):\n",
    "#         super(SeparatorNode, self).__init__('sep', *children)\n",
    "\n",
    "#     def evaluate(self, values):\n",
    "#         value = self._children[0].evaluate(values)\n",
    "#         for child in self._children[1:]:\n",
    "#             value = value and child.evaluate(values)\n",
    "\n",
    "#         return value\n",
    "\n",
    "#     def to_string(self):\n",
    "#         string = f'{self._children[0].to_string()}'\n",
    "#         for child in self._children[1:]:\n",
    "#             string += f' {self._operator_symbol} {child.to_string()}'\n",
    "#         return string\n",
    "\n",
    "\n",
    "# test_for_one_mental_models = functools.partial(test_for_mental_models, allow_only_one_mental_model=False)\n",
    "# # test_for_mental_models_type_two = functools.partial(test_for_mental_models, type_one=False)\n",
    "# # test_for_one_mental_models_type_two = functools.partial(test_for_mental_models, type_one=False,\n",
    "# #                                                         allow_only_one_mental_model=True)\n",
    "# generate_random_sep_tree = functools.partial(generate_random_tree, root_node_cls=SeparatorNode)\n",
    "\n",
    "# generate_and_save_trees('./data', 50000, 2, 5,\n",
    "#                         test_for_one_mental_models, generate_random_sep_tree,\n",
    "#                         base_name='and_trees_single_mms_type_I')\n",
    "\n",
    "# encode_mental_models_separated_sentences('./data', 2, 5, 10,\n",
    "#                                          'encoded_and_trees_single_mms_type_I',\n",
    "#                                          'and_trees_single_mms_type_I')\n",
    "# data = load_sentences_and_conclusions('./data', num_variables=5, max_depth=2,\n",
    "#                                       base_name='encoded_and_trees_single_mms_type_I')\n",
    "# sentences, mental_models, input_dictionary, output_dictionary = data\n",
    "\n",
    "# for i in range(10):\n",
    "#     print(sentences[i])\n",
    "#     print(mental_models[i])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 7  5  9  1  6]\n",
      "  [ 8  1  0  0  0]]\n",
      "\n",
      " [[ 7  4  9  3  6]\n",
      "  [ 7  5 10  4  6]]\n",
      "\n",
      " [[ 7  2  9  3  6]\n",
      "  [ 5  0  0  0  0]]\n",
      "\n",
      " [[ 2  0  0  0  0]\n",
      "  [ 7  2 10  1  6]]\n",
      "\n",
      " [[ 8  1  0  0  0]\n",
      "  [ 7  3  9  1  6]]]\n",
      "[[[-1  0  0  0  1]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]]\n",
      "\n",
      " [[ 0  0  0  1  1]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]]\n",
      "\n",
      " [[ 0  0  1  0  1]\n",
      "  [ 0  1  0  0  1]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]]\n",
      "\n",
      " [[ 1  1  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]]\n",
      "\n",
      " [[-1  0  1  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]]]\n",
      "max_input_length 1\n",
      "input (None, 11, 1)\n",
      "input2 (None, 1)\n",
      "embedding_layer (None, 11, 1, 10)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 11, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 11, 1, 10)    110         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 11, 10)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None, 5)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 11, 128), (N 71168       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 128),  68608       input_6[0][0]                    \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 5)      645         lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 140,531\n",
      "Trainable params: 140,531\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1000\n",
      "354/354 [==============================] - 4s 6ms/step - loss: 0.0844 - val_loss: 0.0750\n",
      "Epoch 2/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0590 - val_loss: 0.0547\n",
      "Epoch 3/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.0442 - val_loss: 0.0396\n",
      "Epoch 4/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.0327 - val_loss: 0.0317\n",
      "Epoch 5/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0266 - val_loss: 0.0276\n",
      "Epoch 6/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.0221 - val_loss: 0.0242\n",
      "Epoch 7/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.0184 - val_loss: 0.0187\n",
      "Epoch 8/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0147 - val_loss: 0.0156\n",
      "Epoch 9/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0119 - val_loss: 0.0139\n",
      "Epoch 10/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0099 - val_loss: 0.0118\n",
      "Epoch 11/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0085 - val_loss: 0.0114\n",
      "Epoch 12/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0072 - val_loss: 0.0100\n",
      "Epoch 13/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0062 - val_loss: 0.0079\n",
      "Epoch 14/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.0053 - val_loss: 0.0076\n",
      "Epoch 15/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.0046 - val_loss: 0.0066\n",
      "Epoch 16/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.0040 - val_loss: 0.0058\n",
      "Epoch 17/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.0034 - val_loss: 0.0050\n",
      "Epoch 18/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.0031 - val_loss: 0.0056\n",
      "Epoch 19/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0030 - val_loss: 0.0050\n",
      "Epoch 20/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.0024 - val_loss: 0.0040\n",
      "Epoch 21/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.0020 - val_loss: 0.0036\n",
      "Epoch 22/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0018 - val_loss: 0.0034\n",
      "Epoch 23/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 24/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0018 - val_loss: 0.0030\n",
      "Epoch 25/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0024\n",
      "Epoch 26/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0024\n",
      "Epoch 27/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 28/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 9.2026e-04 - val_loss: 0.0017\n",
      "Epoch 29/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 7.5451e-04 - val_loss: 0.0018\n",
      "Epoch 30/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 7.4765e-04 - val_loss: 0.0018\n",
      "Epoch 31/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 8.1110e-04 - val_loss: 0.0026\n",
      "Epoch 32/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 33/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 6.3929e-04 - val_loss: 0.0013\n",
      "Epoch 34/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 5.3559e-04 - val_loss: 0.0013\n",
      "Epoch 35/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 5.5494e-04 - val_loss: 0.0014\n",
      "Epoch 36/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 37/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 4.0695e-04 - val_loss: 8.9693e-04\n",
      "Epoch 38/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.2889e-04 - val_loss: 8.2443e-04\n",
      "Epoch 39/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.7685e-04 - val_loss: 9.7782e-04\n",
      "Epoch 40/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.5397e-04 - val_loss: 9.2676e-04\n",
      "Epoch 41/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.4821e-04 - val_loss: 9.9221e-04\n",
      "Epoch 42/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.0014 - val_loss: 9.9253e-04\n",
      "Epoch 43/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 3.4093e-04 - val_loss: 6.7297e-04\n",
      "Epoch 44/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 2.2619e-04 - val_loss: 6.2452e-04\n",
      "Epoch 45/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 2.3492e-04 - val_loss: 6.6193e-04\n",
      "Epoch 46/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.0280e-04 - val_loss: 8.0042e-04\n",
      "Epoch 47/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 48/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.1629e-04 - val_loss: 5.2787e-04\n",
      "Epoch 49/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.6287e-04 - val_loss: 5.0244e-04\n",
      "Epoch 50/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.6661e-04 - val_loss: 4.1385e-04\n",
      "Epoch 51/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.6171e-04 - val_loss: 5.8332e-04\n",
      "Epoch 52/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354/354 [==============================] - 2s 5ms/step - loss: 1.6378e-04 - val_loss: 5.0297e-04\n",
      "Epoch 53/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 2.2855e-04 - val_loss: 0.0012\n",
      "Epoch 54/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 2.5038e-04 - val_loss: 5.3124e-04\n",
      "Epoch 55/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.6768e-04 - val_loss: 5.1020e-04\n",
      "Epoch 56/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 57/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.9583e-04 - val_loss: 4.6583e-04\n",
      "Epoch 58/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 1.1473e-04 - val_loss: 4.0670e-04\n",
      "Epoch 59/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 7.9194e-05 - val_loss: 3.7042e-04\n",
      "Epoch 60/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 7.9441e-05 - val_loss: 3.7765e-04\n",
      "Epoch 61/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 7.1754e-05 - val_loss: 3.4184e-04\n",
      "Epoch 62/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 7.6387e-05 - val_loss: 3.8732e-04\n",
      "Epoch 63/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 8.8489e-05 - val_loss: 3.2203e-04\n",
      "Epoch 64/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 8.8264e-05 - val_loss: 3.4577e-04\n",
      "Epoch 65/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 8.9570e-05 - val_loss: 3.2736e-04\n",
      "Epoch 66/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 9.2540e-05 - val_loss: 8.9572e-04\n",
      "Epoch 67/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 0.0024\n",
      "Epoch 68/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 7.3371e-04 - val_loss: 8.4987e-04\n",
      "Epoch 69/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 1.2133e-04 - val_loss: 3.0090e-04\n",
      "Epoch 70/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 5.5694e-05 - val_loss: 3.2008e-04\n",
      "Epoch 71/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 4.0789e-05 - val_loss: 3.1045e-04\n",
      "Epoch 72/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.6546e-05 - val_loss: 2.6609e-04\n",
      "Epoch 73/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.3811e-05 - val_loss: 2.8523e-04\n",
      "Epoch 74/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.2834e-05 - val_loss: 2.8955e-04\n",
      "Epoch 75/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.1646e-05 - val_loss: 2.9756e-04\n",
      "Epoch 76/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 5.6893e-05 - val_loss: 9.2654e-04\n",
      "Epoch 77/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 0.0014 - val_loss: 8.3251e-04\n",
      "Epoch 78/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 9.0820e-05 - val_loss: 4.5070e-04\n",
      "Epoch 79/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.7215e-05 - val_loss: 4.1525e-04\n",
      "Epoch 80/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 2.7835e-05 - val_loss: 3.8133e-04\n",
      "Epoch 81/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 2.4247e-05 - val_loss: 3.6510e-04\n",
      "Epoch 82/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 2.0910e-05 - val_loss: 3.8004e-04\n",
      "Epoch 83/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 2.4692e-05 - val_loss: 3.6211e-04\n",
      "Epoch 84/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 2.7967e-05 - val_loss: 3.4308e-04\n",
      "Epoch 85/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 2.5012e-05 - val_loss: 3.4169e-04\n",
      "Epoch 86/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 9.9372e-04 - val_loss: 0.0017\n",
      "Epoch 87/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 4.8039e-04 - val_loss: 4.6045e-04\n",
      "Epoch 88/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 2.4639e-04 - val_loss: 4.0595e-04\n",
      "Epoch 89/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 4.3406e-05 - val_loss: 3.5384e-04\n",
      "Epoch 90/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 2.7215e-05 - val_loss: 3.4919e-04\n",
      "Epoch 91/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 2.0409e-05 - val_loss: 2.7297e-04\n",
      "Epoch 92/1000\n",
      "354/354 [==============================] - 2s 4ms/step - loss: 1.6767e-05 - val_loss: 3.2495e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[-1.  0.  1.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[-1.  0.  1.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 1.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 1.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0.  1. -1.  0.  0.]\n",
      " [ 1.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0.  1. -1.  0.  0.]\n",
      " [ 1.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[ 0. -1.  0.  1.  0.]\n",
      " [ 1. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0. -1.  0.  1.  0.]\n",
      " [ 1. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 1. -1.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 1. -1.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[1. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 1.  1.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 1.  1.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[1. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 0.]\n",
      " [1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 0.]\n",
      " [1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 1.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 1.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 1. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 1. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 1.]\n",
      " [1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 1.]\n",
      " [1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[-1.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[-1.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 1.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 1.]\n",
      " [1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 0. 1.]\n",
      " [1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0.  0. -1.  1.]\n",
      " [ 0.  1.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0.  0.  0. -1.  1.]\n",
      " [ 0.  1.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 1.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 1.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 1.]\n",
      " [1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 0. 1.]\n",
      " [1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 1.  0.  0.  1. -1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 1.  0.  0.  1. -1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 1.]\n",
      " [1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 1.]\n",
      " [1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0. -1.  1.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0.  0. -1.  1.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 1.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 1.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0.  0. -1.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0.  0.  0. -1.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[1. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0.  1. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0.  1. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0.  0. -1.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0.  0.  0. -1.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 1.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 1.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 1.  0.  0. -1.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 1.  0.  0. -1.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 1.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 1.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0. -1.  1.  0.]\n",
      " [ 1.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0.  0. -1.  1.  0.]\n",
      " [ 1.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[-1.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[-1.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0. -1.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0.  0. -1.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0.  0. -1.  1.]\n",
      " [ 1.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0.  0.  0. -1.  1.]\n",
      " [ 1.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 1.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0. -1.  0.  1.  0.]\n",
      " [ 0. -1.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0. -1.  0.  1.  0.]\n",
      " [ 0. -1.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[1. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 1.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 1.  0.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 1.]\n",
      " [1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 1. 0.]\n",
      " [1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 1. 0.]\n",
      " [1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0. -1.  0.  0.  1.]\n",
      " [ 0. -1.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0. -1.  0.  0.  1.]\n",
      " [ 0. -1.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[-1.  0.  0.  1.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[-1.  0.  0.  1.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0.  0.  1. -1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0.  0.  0.  1. -1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0.  0. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[ 0. -1.  1.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0. -1.  1.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[1. 1. 0. 0. 1.]\n",
      " [1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 0. 1.]\n",
      " [1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 0. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0. -1.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0.  0. -1.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0. -1.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0. -1.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 1. 1.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 1.  0.  1. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 1.  0.  1. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[1. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 1.  1. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 1.  1. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[ 1.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 1.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[0. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0. 1.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 0. 1.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0.  0.  1. -1.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0.  0.  1. -1.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[ 0.  1. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0.  1. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[ 0.  1.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]] [[ 0.  1.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 1. 0. 1.]\n",
      " [1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]] [[1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "errors:\n",
      "errors 0\n",
      "accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "def broadcast(x, y):\n",
    "    tf.print(x.shape, y.shape)\n",
    "    x = x[..., np.newaxis]\n",
    "    y = y[..., np.newaxis]\n",
    "    x = np.transpose(x, axes=[0, 2, 1])\n",
    "    y = np.transpose(y, axes=[2, 0, 1])\n",
    "    x, y = np.broadcast_arrays(x, y)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def calculate_values(x, y):\n",
    "    s = x + y\n",
    "    sc = np.clip(s, -1, 1)\n",
    "    return sc\n",
    "\n",
    "\n",
    "def calculate_correctness(x, y):\n",
    "    diff = 1 - np.maximum(0, np.abs(x - y) - 1)\n",
    "    prod = np.prod(diff, axis=-1)\n",
    "    return prod\n",
    "\n",
    "\n",
    "def calculate_values_soft(x, y, av=10):\n",
    "    return np.tanh((x + y) * av)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def calculate_correctness_soft(x, y, ac=10):\n",
    "    diff = 1 - sigmoid((np.abs(x - y) - 1.5) * ac)\n",
    "    prod = np.prod(diff, axis=-1)\n",
    "    return prod\n",
    "\n",
    "\n",
    "def calculate_out(values, correctness):\n",
    "    result = values * correctness[..., np.newaxis]\n",
    "    reshaped = np.reshape(result, (result.shape[0] * result.shape[1], result.shape[2]))\n",
    "    return reshaped\n",
    "\n",
    "\n",
    "def combine_mental_models(mm1, mm2):\n",
    "    mm1b, mm2b = broadcast(mm1, mm2)\n",
    "    values = calculate_values(mm1b, mm2b)\n",
    "    correctness = calculate_correctness(mm1b, mm2b)\n",
    "    out = calculate_out(values, correctness)\n",
    "    return out\n",
    "\n",
    "\n",
    "def combine_mental_models_soft(mm1, mm2):\n",
    "    mm1b, mm2b = broadcast(mm1, mm2)\n",
    "    values = calculate_values_soft(mm1b, mm2b, av=10)\n",
    "    correctness = calculate_correctness_soft(mm1b, mm2b, ac=10)\n",
    "    out = calculate_out(values, correctness)\n",
    "    return out\n",
    "\n",
    "\n",
    "def test_mm_inference():\n",
    "    # (a or b)      ---> [T, n], [n, T]\n",
    "    # (a or not b)  ---> [T, n], [n, F]\n",
    "    mm1 = np.array([\n",
    "        [1, 0],\n",
    "        [0, 1],\n",
    "    ])\n",
    "    mm2 = np.array([\n",
    "        [1, 0],\n",
    "        [0, -1]\n",
    "    ])\n",
    "\n",
    "    combined_mental_models = combine_mental_models(mm1, mm2)\n",
    "    combined_mental_models_soft = combine_mental_models_soft(mm1, mm2)\n",
    "    print(combined_mental_models)\n",
    "    print(combined_mental_models_soft)\n",
    "\n",
    "\n",
    "class MMInferenceLayer(kr.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def broadcast(self, x, y):\n",
    "        x = tf.expand_dims(x, axis=-1)\n",
    "        y = tf.expand_dims(y, axis=-1)\n",
    "        x = tf.transpose(x, perm=[0, 1, 3, 2])\n",
    "        y = tf.transpose(y, perm=[0, 3, 1, 2])\n",
    "        # x = tf.broadcast_to(x, (x.shape[0], x.shape[1]))\n",
    "        # x, y =  np.broadcast_arrays(x, y)\n",
    "        return x, y\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = inputs[0]\n",
    "        y = inputs[1]\n",
    "\n",
    "        x, y = self.broadcast(x, y)\n",
    "\n",
    "        s = x + y\n",
    "        value = tf.clip_by_value(s, -1, 1)\n",
    "        # applicability = (tf.reduce_max(tf.abs(x), axis=-1) * tf.reduce_max(tf.abs(y), axis=-1))\n",
    "        # value = value * tf.expand_dims(applicability, axis=-1)\n",
    "\n",
    "        diff = 1 - tf.maximum(0., tf.abs(x - y) - 1.)\n",
    "        correctness = tf.reduce_prod(diff, axis=-1)\n",
    "        mms = value * tf.expand_dims(correctness, axis=-1)\n",
    "        reshaped_value = tf.reshape(mms, (-1, mms.shape[-3] * mms.shape[-2], mms.shape[-1]))\n",
    "        reshaped_correctness = tf.reshape(correctness, (-1, correctness.shape[-2] * correctness.shape[-1]))\n",
    "        mm = tf.reduce_sum(reshaped_value, axis=-2)\n",
    "        mm = mm / tf.reduce_sum(reshaped_correctness, axis=-1, keepdims=True)\n",
    "        # mm = tf.clip_by_value(mm, -1, 1)\n",
    "        # mm = tf.tanh(mm)\n",
    "        return mm\n",
    "\n",
    "\n",
    "def create_inference_model(num_variables, max_input_length, max_sub_mental_models):\n",
    "    embedding_size = 10\n",
    "    hidden_units = 128\n",
    "    print('max_input_length', max_input_length)\n",
    "    input = kr.Input(shape=(2, max_input_length))\n",
    "    split_layer = kr.layers.Lambda(lambda x: (x[:, 0], x[:, 1]))(input)\n",
    "\n",
    "    nn_input = kr.Input(max_input_length)\n",
    "    nn_embedding_layer = kr.layers.Embedding(num_symbols + 1, embedding_size)(nn_input);print(nn_embedding_layer)\n",
    "    flatten_layer = kr.layers.Flatten()(nn_embedding_layer);print(flatten_layer.shape)\n",
    "    nn_hidden = kr.layers.Dense(hidden_units, activation='relu')(flatten_layer)\n",
    "    nn_output = kr.layers.Dense(num_variables * max_sub_mental_models,\n",
    "                                activation='tanh',\n",
    "                                activity_regularizer=kr.regularizers.L1(0.0))(nn_hidden)\n",
    "    nn_reshape = kr.layers.Reshape((max_sub_mental_models, num_variables))(nn_output)\n",
    "    sub_sequence_nn = kr.Model(inputs=nn_input, outputs=nn_reshape, name='sub-sequence-NN')\n",
    "    sub_sequence_nn.summary()\n",
    "\n",
    "    mm = sub_sequence_nn(split_layer[0]), sub_sequence_nn(split_layer[1])\n",
    "    mm_inference_layer = MMInferenceLayer()(mm)\n",
    "\n",
    "    model = kr.Model(inputs=input, outputs=mm_inference_layer)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_varying_inference_model1(num_variables, max_input_length):\n",
    "    # Without specific token for start of sequence (index 0) and end of sequence (index 1) - (0,0,0,0,0) equals end\n",
    "    # Initialise parameters\n",
    "    batch_size = 8\n",
    "    embedding_size = 10\n",
    "    hidden_units = 128\n",
    "    print('max_input_length', max_input_length)\n",
    "    \n",
    "    # Create input for encoder\n",
    "    encoder_inputs = kr.Input(shape=(2, max_input_length))\n",
    "\n",
    "    print('input',encoder_inputs.shape)\n",
    "    \n",
    "    # Make model - Encoder (flatten / concatenate subsentences to one vector (subsentence representation))\n",
    "    nn_input = kr.Input(shape=(num_variables))\n",
    "    nn_embedding_layer = kr.layers.Embedding(num_symbols+1, embedding_size)(encoder_inputs)\n",
    "    nn_flatten = tf.keras.layers.Reshape((nn_embedding_layer.shape[1],-1))(nn_embedding_layer)\n",
    "    encoder = kr.layers.LSTM(hidden_units, return_sequences=True, return_state=True, activation='relu')\n",
    "    encoder_outputs, state_h, state_c = encoder(nn_flatten)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Create decoder\n",
    "    decoder_inputs = kr.Input(shape=(None,num_variables))\n",
    "    decoder = kr.layers.LSTM(hidden_units, return_sequences=True, return_state=True, activation='relu')\n",
    "    decoder_outputs, _, _ = decoder(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = kr.layers.Dense(num_variables, activation='tanh')\n",
    "    output = decoder_dense(decoder_outputs)\n",
    "\n",
    "    ## Define training model\n",
    "    model_train = kr.Model(inputs=[encoder_inputs, decoder_inputs], outputs=output)\n",
    "    model_train.summary()\n",
    "    \n",
    "    ## Train model \n",
    "    model_train.compile(optimizer=kr.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=kr.losses.mse)\n",
    "\n",
    "    callbacks = [kr.callbacks.EarlyStopping(patience=20, min_delta=1e-5, restore_best_weights=True)]\n",
    "    history = model_train.fit([ds.x_train, ds.y_train_d], ds.y_train, validation_data=([ds.x_valid, ds.y_valid_d], ds.y_valid),\n",
    "                        epochs=1000, batch_size=batch_size, callbacks=callbacks)\n",
    "    \n",
    "    ## Define testing models (no teacher forcing)\n",
    "    encoder_model = kr.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = kr.Input(shape=(hidden_units,))\n",
    "    decoder_state_input_c = kr.Input(shape=(hidden_units,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = kr.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "\n",
    "    # Returned trained models, and history of training\n",
    "    return model_train, history, encoder_model, decoder_model\n",
    "\n",
    "def create_varying_inference_model2(num_variables, max_input_length):\n",
    "    # With specific token for start of sequence (index 0) and end of sequence (index 1)\n",
    "    # Initialise parameters\n",
    "    batch_size = 8\n",
    "    embedding_size = 10\n",
    "    hidden_units = 128\n",
    "    print('max_input_length', max_input_length)\n",
    "    # Create input for encoder\n",
    "    encoder_inputs = kr.Input(shape=(2, max_input_length))\n",
    "\n",
    "    print('input',encoder_inputs.shape)\n",
    "    \n",
    "    # Make model - Encoder (flatten / concatenate subsentences to one vector (subsentence representation))\n",
    "    nn_input = kr.Input(shape=(num_variables))\n",
    "    nn_embedding_layer = kr.layers.Embedding(num_symbols+1, embedding_size)(encoder_inputs)\n",
    "    nn_flatten = tf.keras.layers.Reshape((nn_embedding_layer.shape[1],-1))(nn_embedding_layer)\n",
    "    encoder = kr.layers.LSTM(hidden_units, return_sequences=True, return_state=True, activation='relu')\n",
    "    encoder_outputs, state_h, state_c = encoder(nn_flatten)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Create decoder\n",
    "    decoder_inputs = kr.Input(shape=(None,num_variables+2))\n",
    "    decoder = kr.layers.LSTM(hidden_units, return_sequences=True, return_state=True, activation='relu')\n",
    "    decoder_outputs, _, _ = decoder(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = kr.layers.Dense(num_variables+2, activation='tanh')\n",
    "    output = decoder_dense(decoder_outputs)\n",
    "\n",
    "    ## Define training model\n",
    "    model_train = kr.Model(inputs=[encoder_inputs, decoder_inputs], outputs=output)\n",
    "    model_train.summary()\n",
    "    \n",
    "    ## Train model \n",
    "    model_train.compile(optimizer=kr.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=kr.losses.mse)\n",
    "\n",
    "    callbacks = [kr.callbacks.EarlyStopping(patience=20, min_delta=1e-5, restore_best_weights=True)]\n",
    "    history = model_train.fit([ds.x_train, ds.y_train_d], ds.y_train, validation_data=([ds.x_valid, ds.y_valid_d], ds.y_valid),\n",
    "                        epochs=1000, batch_size=batch_size, callbacks=callbacks)\n",
    "    \n",
    "    ## Define testing models (no teacher forcing)\n",
    "    encoder_model = kr.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = kr.Input(shape=(hidden_units,))\n",
    "    decoder_state_input_c = kr.Input(shape=(hidden_units,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = kr.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "\n",
    "    # Returned trained models, and history of training\n",
    "    return model_train, history, encoder_model, decoder_model\n",
    "\n",
    "def create_varying_inference_model3(num_variables, max_input_length):\n",
    "    # Without specific token for start of sequence (index 0) and end of sequence (index 1) - (0,0,0,0,0) equals end\n",
    "    # Based on characters instead of subsentences\n",
    "    # Initialise parameters\n",
    "    batch_size = 8\n",
    "    embedding_size = 10\n",
    "    hidden_units = 128\n",
    "    print('max_input_length', max_input_length)\n",
    "    \n",
    "    # Create input for encoder\n",
    "    encoder_inputs = kr.Input(shape=(11, max_input_length))\n",
    "\n",
    "    print('input',encoder_inputs.shape)\n",
    "    \n",
    "    # Make model - Encoder (flatten / concatenate subsentences to one vector (subsentence representation))\n",
    "    nn_input = kr.Input(shape=(1))\n",
    "    print('input2', nn_input.shape)\n",
    "    nn_embedding_layer = kr.layers.Embedding(num_symbols+1, embedding_size)(encoder_inputs)\n",
    "    nn_flatten = tf.keras.layers.Reshape((nn_embedding_layer.shape[1],-1))(nn_embedding_layer)\n",
    "    print('embedding_layer', nn_embedding_layer.shape)\n",
    "    encoder = kr.layers.LSTM(hidden_units, return_sequences=True, return_state=True, activation='relu')\n",
    "    encoder_outputs, state_h, state_c = encoder(nn_flatten)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Create decoder\n",
    "    decoder_inputs = kr.Input(shape=(None,num_variables))\n",
    "    decoder = kr.layers.LSTM(hidden_units, return_sequences=True, return_state=True, activation='relu')\n",
    "    decoder_outputs, _, _ = decoder(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = kr.layers.Dense(num_variables, activation='tanh')\n",
    "    output = decoder_dense(decoder_outputs)\n",
    "\n",
    "    ## Define training model\n",
    "    model_train = kr.Model(inputs=[encoder_inputs, decoder_inputs], outputs=output)\n",
    "    model_train.summary()\n",
    "    \n",
    "    ## Train model \n",
    "    model_train.compile(optimizer=kr.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=kr.losses.mse)\n",
    "\n",
    "    callbacks = [kr.callbacks.EarlyStopping(patience=20, min_delta=1e-5, restore_best_weights=True)]\n",
    "    history = model_train.fit([ds.x_train, ds.y_train_d], ds.y_train, validation_data=([ds.x_valid, ds.y_valid_d], ds.y_valid),\n",
    "                        epochs=1000, batch_size=batch_size, callbacks=callbacks)\n",
    "    \n",
    "    ## Define testing models (no teacher forcing)\n",
    "    encoder_model = kr.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "    decoder_state_input_h = kr.Input(shape=(hidden_units,))\n",
    "    decoder_state_input_c = kr.Input(shape=(hidden_units,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = kr.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "\n",
    "    # Returned trained models, and history of training\n",
    "    return model_train, history, encoder_model, decoder_model\n",
    "\n",
    "def decode_sequence1(input_seq, encoder_model, decoder_model):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_variables))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "#     target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_output = target_seq\n",
    "    while not stop_condition:\n",
    "        output, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Save MMs\n",
    "        pred = np.rint(output).astype(int)\n",
    "        decoded_output = np.concatenate((decoded_output, pred), axis=1)\n",
    "\n",
    "        # Exit condition: hit max length\n",
    "        # this padding, such that all arrays have the same size in decoded_output.\n",
    "        if decoded_output.shape[1] > num_variables:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = pred\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_output[:,1:,:]\n",
    "\n",
    "def decode_sequence2(input_seq, encoder_model, decoder_model):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_variables+2))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "#     target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_output = target_seq\n",
    "    while not stop_condition:\n",
    "        output, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Save MMs\n",
    "        pred = np.rint(output).astype(int)\n",
    "        decoded_output = np.concatenate((decoded_output, pred), axis=1)\n",
    "\n",
    "        # Exit condition: hit max length\n",
    "        # this padding, such that all arrays have the same size in decoded_output.\n",
    "        if decoded_output.shape[1] > num_variables:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = pred\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_output[:,1:,:]\n",
    "\n",
    "def decode_sequences1(data, encoder_model, decoder_model):\n",
    "    preds = decode_sequence1(data[[0]], encoder_model, decoder_model)\n",
    "    for i in range(1,ds.x_test.shape[0]):\n",
    "        pred = decode_sequence1(data[[i]], encoder_model, decoder_model)\n",
    "        preds = np.concatenate((preds, pred), axis=0)\n",
    "        print(i)\n",
    "        \n",
    "    return preds\n",
    "\n",
    "def decode_sequences2(data, encoder_model, decoder_model):\n",
    "    preds = decode_sequence2(data[[0]], encoder_model, decoder_model)\n",
    "    for i in range(1,ds.x_test.shape[0]):\n",
    "        pred = decode_sequence2(data[[i]], encoder_model, decoder_model)\n",
    "        preds = np.concatenate((preds, pred), axis=0)\n",
    "        print(i)\n",
    "        \n",
    "    return preds\n",
    "\n",
    "def two_way_mse(y_true, y_pred):\n",
    "    y_true_float = tf.cast(y_true, y_pred.dtype)\n",
    "    diff = (y_true_float - y_pred) ** 2\n",
    "    print(diff)\n",
    "    return tf.reduce_mean(diff)\n",
    "\n",
    "\n",
    "def show_subsentence_inference(model, ds, decoding_dictionary, idxs):\n",
    "    sub_model = model.layers[2]\n",
    "    for i in idxs:\n",
    "        for j in range(2):\n",
    "            x = ds.x_test[i][j]\n",
    "            pred = sub_model.predict(x[np.newaxis, ...])\n",
    "            print(dataset.encoding.decode_sentence(x, decoding_dictionary, ds.indexed_encoding))\n",
    "            print(np.rint(pred))\n",
    "            \n",
    "def add_zero_row1(data, position):\n",
    "    if position == 'front':\n",
    "        temp = np.zeros((data.shape[0],data.shape[1]+1,data.shape[2]))\n",
    "        temp[:,1:,:] = data\n",
    "    elif position == 'last':\n",
    "        temp = np.zeros((data.shape[0],data.shape[1]+1,data.shape[2]))\n",
    "        temp[:,:-1,:] = data\n",
    "\n",
    "    return temp\n",
    "\n",
    "def add_zero_row2(dst, position):\n",
    "    start_vec = np.array([1,0] + [0] * (num_variables))\n",
    "    end_vec = np.array([0,1] + [0] * (num_variables))\n",
    "    if position == 'front':\n",
    "        data = np.zeros((dst.shape[0], dst.shape[1]+1, dst.shape[2]+2))\n",
    "        data[:,1:,2:] = dst\n",
    "        for i in range(data.shape[0]):\n",
    "            data[i][data[i].sum(axis=1) == 0] = end_vec\n",
    "            data[i][0,:] = start_vec\n",
    "    elif position == 'last':\n",
    "        data = np.zeros((dst.shape[0], dst.shape[1]+1, dst.shape[2]+2))\n",
    "        data[:,:-1,2:] = dst\n",
    "        for i in range(data.shape[0]):\n",
    "            data[i][data[i].sum(axis=1) == 0] = end_vec\n",
    "            data[i][-1,:] = end_vec\n",
    "        \n",
    "    return data\n",
    "\n",
    "def concat_subsentences(data):\n",
    "    temp = np.array(data[0][0].tolist() + [10] + data[0][1].tolist())[np.newaxis, ...]\n",
    "    for i in range(1,data.shape[0]):\n",
    "        sentence = np.array(data[i][0].tolist() + [10] + data[i][1].tolist())[np.newaxis, ...]\n",
    "        temp = np.concatenate((temp, sentence), axis=0)\n",
    "    \n",
    "    return temp[..., np.newaxis]\n",
    "\n",
    "def same_MMs(true, pred):\n",
    "    true = true.tolist()\n",
    "    pred = pred.tolist()\n",
    "    # Check if all occur\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] in true:\n",
    "            # If occur, remove from true\n",
    "            true.remove(pred[i])\n",
    "\n",
    "    # If nothing in true, everything is predicted correctly\n",
    "    return true == []\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ds = get_separated_sequences_mental_models_dataset('./data', 'encoded_and_trees_single_mms_type_I',\n",
    "                                                       num_variables=5, max_depth=2,\n",
    "                                                       test_size=.1, valid_size=.1,\n",
    "                                                       indexed_encoding=True, pad_mental_models=True)\n",
    "\n",
    "    dec_in, dec_out = dataset.encoding.create_decoding_dictionaries(ds.input_dictionary, ds.output_dictionary)\n",
    "\n",
    "    ds.y_train_d = add_zero_row1(ds.y_train, 'front')\n",
    "    ds.y_train = add_zero_row1(ds.y_train, 'last')\n",
    "    ds.y_valid_d = add_zero_row1(ds.y_valid, 'front')\n",
    "    ds.y_valid = add_zero_row1(ds.y_valid, 'last')\n",
    "    ds.y_test_d = add_zero_row1(ds.y_test, 'front')\n",
    "    ds.y_test = add_zero_row1(ds.y_test, 'last')\n",
    "    \n",
    "    # Uncomment for model 3\n",
    "    ds.x_train = concat_subsentences(ds.x_train)\n",
    "    ds.x_valid = concat_subsentences(ds.x_valid)\n",
    "    ds.x_test = concat_subsentences(ds.x_test)\n",
    "    \n",
    "    num_variables = 5\n",
    "    num_operators = 5  # and, or, not\n",
    "    num_symbols = num_variables + num_operators\n",
    "    max_input_length = ds.x_train.shape[-1]\n",
    "    \n",
    "    # 1: subsentences no index, 2: subsentences index, 3: symbols no index\n",
    "    model_train, history, encoder_model, decoder_model = create_varying_inference_model3(num_variables, \n",
    "                                                                                        max_input_length)\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    plt.plot(range(len(loss)), loss, label='loss')\n",
    "    plt.plot(range(len(val_loss)), loss, label='val_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    preds = decode_sequences1(ds.x_test, encoder_model, decoder_model)\n",
    "    for i in range(preds.shape[0]):\n",
    "        print(preds[i], ds.y_test[i])\n",
    "\n",
    "    print('errors:')\n",
    "    errors = 0\n",
    "    for i in range(preds.shape[0]):\n",
    "        if same_MMs(ds.y_test[i], preds[i]):\n",
    "            continue\n",
    "        print(dataset.encoding.decode_sentence(ds.x_test[i][0], dec_in, ds.indexed_encoding))\n",
    "        print(dataset.encoding.decode_sentence(ds.x_test[i][1], dec_in, ds.indexed_encoding))\n",
    "        print(ds.y_test[i])\n",
    "        print(preds[i])\n",
    "        print()\n",
    "        \n",
    "        errors += 1\n",
    "\n",
    "    errors = np.count_nonzero(np.sum(np.abs(preds - ds.y_test), axis=-1))\n",
    "    print('errors', int(errors))\n",
    "    print(f'accuracy: {int((1 - int(errors)/ds.x_test.shape[0]) * 1000) / 10}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
