{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.getcwd() + '\\\\temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 100 (0.2%) trees, correct: 90, recently correct: 90.0%, eta: 04s\n",
      "Checked 200 (0.4%) trees, correct: 178, recently correct: 88.0%, eta: 04s\n",
      "Checked 300 (0.6%) trees, correct: 258, recently correct: 80.0%, eta: 04s\n",
      "Checked 400 (0.8%) trees, correct: 334, recently correct: 76.0%, eta: 03s\n",
      "Checked 500 (1.0%) trees, correct: 403, recently correct: 69.0%, eta: 04s\n",
      "Checked 600 (1.2%) trees, correct: 468, recently correct: 65.0%, eta: 04s\n",
      "Checked 700 (1.4%) trees, correct: 528, recently correct: 60.0%, eta: 04s\n",
      "Checked 800 (1.6%) trees, correct: 588, recently correct: 60.0%, eta: 04s\n",
      "Checked 900 (1.8%) trees, correct: 640, recently correct: 52.0%, eta: 04s\n",
      "Checked 1000 (2.0%) trees, correct: 702, recently correct: 62.0%, eta: 04s\n",
      "Checked 1100 (2.2%) trees, correct: 751, recently correct: 49.0%, eta: 04s\n",
      "Checked 1200 (2.4%) trees, correct: 808, recently correct: 57.0%, eta: 04s\n",
      "Checked 1300 (2.6%) trees, correct: 855, recently correct: 47.0%, eta: 04s\n",
      "Checked 1400 (2.8%) trees, correct: 897, recently correct: 42.0%, eta: 04s\n",
      "Checked 1500 (3.0%) trees, correct: 943, recently correct: 46.0%, eta: 04s\n",
      "Checked 1600 (3.2%) trees, correct: 997, recently correct: 54.0%, eta: 04s\n",
      "Checked 1700 (3.4%) trees, correct: 1044, recently correct: 47.0%, eta: 04s\n",
      "Checked 1800 (3.6%) trees, correct: 1083, recently correct: 39.0%, eta: 04s\n",
      "Checked 1900 (3.8%) trees, correct: 1123, recently correct: 40.0%, eta: 04s\n",
      "Checked 2000 (4.0%) trees, correct: 1170, recently correct: 47.0%, eta: 04s\n",
      "Checked 2100 (4.2%) trees, correct: 1212, recently correct: 42.0%, eta: 04s\n",
      "Checked 2200 (4.4%) trees, correct: 1259, recently correct: 47.0%, eta: 04s\n",
      "Checked 2300 (4.6%) trees, correct: 1304, recently correct: 45.0%, eta: 04s\n",
      "Checked 2400 (4.8%) trees, correct: 1342, recently correct: 38.0%, eta: 04s\n",
      "Checked 2500 (5.0%) trees, correct: 1381, recently correct: 39.0%, eta: 04s\n",
      "Checked 2600 (5.2%) trees, correct: 1412, recently correct: 31.0%, eta: 04s\n",
      "Checked 2700 (5.4%) trees, correct: 1449, recently correct: 37.0%, eta: 04s\n",
      "Checked 2800 (5.6%) trees, correct: 1478, recently correct: 29.0%, eta: 04s\n",
      "Checked 2900 (5.8%) trees, correct: 1503, recently correct: 25.0%, eta: 04s\n",
      "Checked 3000 (6.0%) trees, correct: 1533, recently correct: 30.0%, eta: 04s\n",
      "Checked 3100 (6.2%) trees, correct: 1573, recently correct: 40.0%, eta: 04s\n",
      "Checked 3200 (6.4%) trees, correct: 1612, recently correct: 39.0%, eta: 04s\n",
      "Checked 3300 (6.6%) trees, correct: 1648, recently correct: 36.0%, eta: 04s\n",
      "Checked 3400 (6.8%) trees, correct: 1687, recently correct: 39.0%, eta: 04s\n",
      "Checked 3500 (7.0%) trees, correct: 1713, recently correct: 26.0%, eta: 04s\n",
      "Checked 3600 (7.2%) trees, correct: 1746, recently correct: 33.0%, eta: 04s\n",
      "Checked 3700 (7.4%) trees, correct: 1775, recently correct: 29.0%, eta: 04s\n",
      "Checked 3800 (7.6%) trees, correct: 1801, recently correct: 26.0%, eta: 04s\n",
      "Checked 3900 (7.8%) trees, correct: 1824, recently correct: 23.0%, eta: 04s\n",
      "Checked 4000 (8.0%) trees, correct: 1858, recently correct: 34.0%, eta: 04s\n",
      "Checked 4100 (8.2%) trees, correct: 1891, recently correct: 33.0%, eta: 04s\n",
      "Checked 4200 (8.4%) trees, correct: 1919, recently correct: 28.0%, eta: 04s\n",
      "Checked 4300 (8.6%) trees, correct: 1945, recently correct: 26.0%, eta: 04s\n",
      "Checked 4400 (8.8%) trees, correct: 1960, recently correct: 15.0%, eta: 04s\n",
      "Checked 4500 (9.0%) trees, correct: 1987, recently correct: 27.0%, eta: 04s\n",
      "Checked 4600 (9.2%) trees, correct: 2009, recently correct: 22.0%, eta: 04s\n",
      "Checked 4700 (9.4%) trees, correct: 2026, recently correct: 17.0%, eta: 04s\n",
      "Checked 4800 (9.6%) trees, correct: 2039, recently correct: 13.0%, eta: 04s\n",
      "Checked 4900 (9.8%) trees, correct: 2065, recently correct: 26.0%, eta: 04s\n",
      "Checked 5000 (10.0%) trees, correct: 2089, recently correct: 24.0%, eta: 04s\n",
      "Checked 5100 (10.2%) trees, correct: 2105, recently correct: 16.0%, eta: 04s\n",
      "Checked 5200 (10.4%) trees, correct: 2125, recently correct: 20.0%, eta: 04s\n",
      "Checked 5300 (10.6%) trees, correct: 2138, recently correct: 13.0%, eta: 04s\n",
      "Checked 5400 (10.8%) trees, correct: 2160, recently correct: 22.0%, eta: 04s\n",
      "Checked 5500 (11.0%) trees, correct: 2183, recently correct: 23.0%, eta: 04s\n",
      "Checked 5600 (11.2%) trees, correct: 2204, recently correct: 21.0%, eta: 04s\n",
      "Checked 5700 (11.4%) trees, correct: 2219, recently correct: 15.0%, eta: 04s\n",
      "Checked 5800 (11.6%) trees, correct: 2235, recently correct: 16.0%, eta: 04s\n",
      "Checked 5900 (11.8%) trees, correct: 2254, recently correct: 19.0%, eta: 04s\n",
      "Checked 6000 (12.0%) trees, correct: 2272, recently correct: 18.0%, eta: 04s\n",
      "Checked 6100 (12.2%) trees, correct: 2287, recently correct: 15.0%, eta: 04s\n",
      "Checked 6200 (12.4%) trees, correct: 2302, recently correct: 15.0%, eta: 04s\n",
      "Checked 6300 (12.6%) trees, correct: 2324, recently correct: 22.0%, eta: 04s\n",
      "Checked 6400 (12.8%) trees, correct: 2341, recently correct: 17.0%, eta: 04s\n",
      "Checked 6500 (13.0%) trees, correct: 2362, recently correct: 21.0%, eta: 04s\n",
      "Checked 6600 (13.2%) trees, correct: 2382, recently correct: 20.0%, eta: 04s\n",
      "Checked 6700 (13.4%) trees, correct: 2399, recently correct: 17.0%, eta: 04s\n",
      "Checked 6800 (13.6%) trees, correct: 2414, recently correct: 15.0%, eta: 04s\n",
      "Checked 6900 (13.8%) trees, correct: 2428, recently correct: 14.0%, eta: 04s\n",
      "Checked 7000 (14.0%) trees, correct: 2448, recently correct: 20.0%, eta: 04s\n",
      "Checked 7100 (14.2%) trees, correct: 2469, recently correct: 21.0%, eta: 04s\n",
      "Checked 7200 (14.4%) trees, correct: 2481, recently correct: 12.0%, eta: 04s\n",
      "Checked 7300 (14.6%) trees, correct: 2492, recently correct: 11.0%, eta: 04s\n",
      "Checked 7400 (14.8%) trees, correct: 2504, recently correct: 12.0%, eta: 04s\n",
      "Checked 7500 (15.0%) trees, correct: 2521, recently correct: 17.0%, eta: 04s\n",
      "Checked 7600 (15.2%) trees, correct: 2535, recently correct: 14.0%, eta: 04s\n",
      "Checked 7700 (15.4%) trees, correct: 2549, recently correct: 14.0%, eta: 04s\n",
      "Checked 7800 (15.6%) trees, correct: 2560, recently correct: 11.0%, eta: 04s\n",
      "Checked 7900 (15.8%) trees, correct: 2576, recently correct: 16.0%, eta: 04s\n",
      "Checked 8000 (16.0%) trees, correct: 2590, recently correct: 14.0%, eta: 04s\n",
      "Checked 8100 (16.2%) trees, correct: 2603, recently correct: 13.0%, eta: 04s\n",
      "Checked 8200 (16.4%) trees, correct: 2620, recently correct: 17.0%, eta: 04s\n",
      "Checked 8300 (16.6%) trees, correct: 2630, recently correct: 10.0%, eta: 04s\n",
      "Checked 8400 (16.8%) trees, correct: 2644, recently correct: 14.0%, eta: 04s\n",
      "Checked 8500 (17.0%) trees, correct: 2660, recently correct: 16.0%, eta: 04s\n",
      "Checked 8600 (17.2%) trees, correct: 2675, recently correct: 15.0%, eta: 04s\n",
      "Checked 8700 (17.4%) trees, correct: 2695, recently correct: 20.0%, eta: 04s\n",
      "Checked 8800 (17.6%) trees, correct: 2709, recently correct: 14.0%, eta: 04s\n",
      "Checked 8900 (17.8%) trees, correct: 2722, recently correct: 13.0%, eta: 04s\n",
      "Checked 9000 (18.0%) trees, correct: 2732, recently correct: 10.0%, eta: 04s\n",
      "Checked 9100 (18.2%) trees, correct: 2743, recently correct: 11.0%, eta: 04s\n",
      "Checked 9200 (18.4%) trees, correct: 2759, recently correct: 16.0%, eta: 04s\n",
      "Checked 9300 (18.6%) trees, correct: 2772, recently correct: 13.0%, eta: 04s\n",
      "Checked 9400 (18.8%) trees, correct: 2780, recently correct: 8.0%, eta: 04s\n",
      "Checked 9500 (19.0%) trees, correct: 2793, recently correct: 13.0%, eta: 04s\n",
      "Checked 9600 (19.2%) trees, correct: 2804, recently correct: 11.0%, eta: 04s\n",
      "Checked 9700 (19.4%) trees, correct: 2814, recently correct: 10.0%, eta: 04s\n",
      "Checked 9800 (19.6%) trees, correct: 2826, recently correct: 12.0%, eta: 04s\n",
      "Checked 9900 (19.8%) trees, correct: 2838, recently correct: 12.0%, eta: 04s\n",
      "Checked 10000 (20.0%) trees, correct: 2845, recently correct: 7.0%, eta: 04s\n",
      "Checked 10100 (20.2%) trees, correct: 2856, recently correct: 11.0%, eta: 04s\n",
      "Checked 10200 (20.4%) trees, correct: 2866, recently correct: 10.0%, eta: 04s\n",
      "Checked 10300 (20.6%) trees, correct: 2874, recently correct: 8.0%, eta: 04s\n",
      "Checked 10400 (20.8%) trees, correct: 2884, recently correct: 10.0%, eta: 04s\n",
      "Checked 10500 (21.0%) trees, correct: 2890, recently correct: 6.0%, eta: 04s\n",
      "Checked 10600 (21.2%) trees, correct: 2900, recently correct: 10.0%, eta: 04s\n",
      "Checked 10700 (21.4%) trees, correct: 2911, recently correct: 11.0%, eta: 04s\n",
      "Checked 10800 (21.6%) trees, correct: 2920, recently correct: 9.0%, eta: 04s\n",
      "Checked 10900 (21.8%) trees, correct: 2927, recently correct: 7.0%, eta: 04s\n",
      "Checked 11000 (22.0%) trees, correct: 2934, recently correct: 7.0%, eta: 04s\n",
      "Checked 11100 (22.2%) trees, correct: 2940, recently correct: 6.0%, eta: 04s\n",
      "Checked 11200 (22.4%) trees, correct: 2947, recently correct: 7.0%, eta: 04s\n",
      "Checked 11300 (22.6%) trees, correct: 2960, recently correct: 13.0%, eta: 04s\n",
      "Checked 11400 (22.8%) trees, correct: 2971, recently correct: 11.0%, eta: 04s\n",
      "Checked 11500 (23.0%) trees, correct: 2976, recently correct: 5.0%, eta: 04s\n",
      "Checked 11600 (23.2%) trees, correct: 2983, recently correct: 7.0%, eta: 04s\n",
      "Checked 11700 (23.4%) trees, correct: 2991, recently correct: 8.0%, eta: 04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 11800 (23.6%) trees, correct: 2996, recently correct: 5.0%, eta: 04s\n",
      "Checked 11900 (23.8%) trees, correct: 3003, recently correct: 7.0%, eta: 04s\n",
      "Checked 12000 (24.0%) trees, correct: 3009, recently correct: 6.0%, eta: 04s\n",
      "Checked 12100 (24.2%) trees, correct: 3017, recently correct: 8.0%, eta: 04s\n",
      "Checked 12200 (24.4%) trees, correct: 3020, recently correct: 3.0%, eta: 04s\n",
      "Checked 12300 (24.6%) trees, correct: 3027, recently correct: 7.0%, eta: 04s\n",
      "Checked 12400 (24.8%) trees, correct: 3033, recently correct: 6.0%, eta: 04s\n",
      "Checked 12500 (25.0%) trees, correct: 3041, recently correct: 8.0%, eta: 04s\n",
      "Checked 12600 (25.2%) trees, correct: 3050, recently correct: 9.0%, eta: 04s\n",
      "Checked 12700 (25.4%) trees, correct: 3057, recently correct: 7.0%, eta: 04s\n",
      "Checked 12800 (25.6%) trees, correct: 3062, recently correct: 5.0%, eta: 04s\n",
      "Checked 12900 (25.8%) trees, correct: 3070, recently correct: 8.0%, eta: 04s\n",
      "Checked 13000 (26.0%) trees, correct: 3076, recently correct: 6.0%, eta: 04s\n",
      "Checked 13100 (26.2%) trees, correct: 3083, recently correct: 7.0%, eta: 04s\n",
      "Checked 13200 (26.4%) trees, correct: 3090, recently correct: 7.0%, eta: 04s\n",
      "Checked 13300 (26.6%) trees, correct: 3094, recently correct: 4.0%, eta: 03s\n",
      "Checked 13400 (26.8%) trees, correct: 3106, recently correct: 12.0%, eta: 03s\n",
      "Checked 13500 (27.0%) trees, correct: 3110, recently correct: 4.0%, eta: 03s\n",
      "Checked 13600 (27.2%) trees, correct: 3113, recently correct: 3.0%, eta: 03s\n",
      "Checked 13700 (27.4%) trees, correct: 3118, recently correct: 5.0%, eta: 03s\n",
      "Checked 13800 (27.6%) trees, correct: 3126, recently correct: 8.0%, eta: 03s\n",
      "Checked 13900 (27.8%) trees, correct: 3132, recently correct: 6.0%, eta: 03s\n",
      "Checked 14000 (28.0%) trees, correct: 3139, recently correct: 7.0%, eta: 03s\n",
      "Checked 14100 (28.2%) trees, correct: 3145, recently correct: 6.0%, eta: 03s\n",
      "Checked 14200 (28.4%) trees, correct: 3149, recently correct: 4.0%, eta: 03s\n",
      "Checked 14300 (28.6%) trees, correct: 3161, recently correct: 12.0%, eta: 03s\n",
      "Checked 14400 (28.8%) trees, correct: 3164, recently correct: 3.0%, eta: 03s\n",
      "Checked 14500 (29.0%) trees, correct: 3170, recently correct: 6.0%, eta: 03s\n",
      "Checked 14600 (29.2%) trees, correct: 3177, recently correct: 7.0%, eta: 03s\n",
      "Checked 14700 (29.4%) trees, correct: 3181, recently correct: 4.0%, eta: 03s\n",
      "Checked 14800 (29.6%) trees, correct: 3184, recently correct: 3.0%, eta: 03s\n",
      "Checked 14900 (29.8%) trees, correct: 3188, recently correct: 4.0%, eta: 03s\n",
      "Checked 15000 (30.0%) trees, correct: 3195, recently correct: 7.0%, eta: 03s\n",
      "Checked 15100 (30.2%) trees, correct: 3198, recently correct: 3.0%, eta: 03s\n",
      "Checked 15200 (30.4%) trees, correct: 3204, recently correct: 6.0%, eta: 03s\n",
      "Checked 15300 (30.6%) trees, correct: 3208, recently correct: 4.0%, eta: 03s\n",
      "Checked 15400 (30.8%) trees, correct: 3211, recently correct: 3.0%, eta: 03s\n",
      "Checked 15500 (31.0%) trees, correct: 3213, recently correct: 2.0%, eta: 03s\n",
      "Checked 15600 (31.2%) trees, correct: 3217, recently correct: 4.0%, eta: 03s\n",
      "Checked 15700 (31.4%) trees, correct: 3222, recently correct: 5.0%, eta: 03s\n",
      "Checked 15800 (31.6%) trees, correct: 3224, recently correct: 2.0%, eta: 03s\n",
      "Checked 15900 (31.8%) trees, correct: 3228, recently correct: 4.0%, eta: 03s\n",
      "Checked 16000 (32.0%) trees, correct: 3232, recently correct: 4.0%, eta: 03s\n",
      "Checked 16100 (32.2%) trees, correct: 3233, recently correct: 1.0%, eta: 03s\n",
      "Checked 16200 (32.4%) trees, correct: 3235, recently correct: 2.0%, eta: 03s\n",
      "Checked 16300 (32.6%) trees, correct: 3241, recently correct: 6.0%, eta: 03s\n",
      "Checked 16400 (32.8%) trees, correct: 3245, recently correct: 4.0%, eta: 03s\n",
      "Checked 16500 (33.0%) trees, correct: 3247, recently correct: 2.0%, eta: 03s\n",
      "Checked 16600 (33.2%) trees, correct: 3253, recently correct: 6.0%, eta: 03s\n",
      "Checked 16700 (33.4%) trees, correct: 3254, recently correct: 1.0%, eta: 03s\n",
      "Checked 16800 (33.6%) trees, correct: 3256, recently correct: 2.0%, eta: 03s\n",
      "Checked 16900 (33.8%) trees, correct: 3259, recently correct: 3.0%, eta: 03s\n",
      "Checked 17000 (34.0%) trees, correct: 3261, recently correct: 2.0%, eta: 03s\n",
      "Checked 17100 (34.2%) trees, correct: 3265, recently correct: 4.0%, eta: 03s\n",
      "Checked 17200 (34.4%) trees, correct: 3269, recently correct: 4.0%, eta: 03s\n",
      "Checked 17300 (34.6%) trees, correct: 3272, recently correct: 3.0%, eta: 03s\n",
      "Checked 17400 (34.8%) trees, correct: 3279, recently correct: 7.0%, eta: 03s\n",
      "Checked 17500 (35.0%) trees, correct: 3282, recently correct: 3.0%, eta: 03s\n",
      "Checked 17600 (35.2%) trees, correct: 3287, recently correct: 5.0%, eta: 03s\n",
      "Checked 17700 (35.4%) trees, correct: 3293, recently correct: 6.0%, eta: 03s\n",
      "Checked 17800 (35.6%) trees, correct: 3295, recently correct: 2.0%, eta: 03s\n",
      "Checked 17900 (35.8%) trees, correct: 3297, recently correct: 2.0%, eta: 03s\n",
      "Checked 18000 (36.0%) trees, correct: 3299, recently correct: 2.0%, eta: 03s\n",
      "Checked 18100 (36.2%) trees, correct: 3300, recently correct: 1.0%, eta: 03s\n",
      "Checked 18200 (36.4%) trees, correct: 3303, recently correct: 3.0%, eta: 03s\n",
      "Checked 18300 (36.6%) trees, correct: 3307, recently correct: 4.0%, eta: 03s\n",
      "Checked 18400 (36.8%) trees, correct: 3311, recently correct: 4.0%, eta: 03s\n",
      "Checked 18500 (37.0%) trees, correct: 3315, recently correct: 4.0%, eta: 03s\n",
      "Checked 18600 (37.2%) trees, correct: 3317, recently correct: 2.0%, eta: 03s\n",
      "Checked 18700 (37.4%) trees, correct: 3319, recently correct: 2.0%, eta: 03s\n",
      "Checked 18800 (37.6%) trees, correct: 3320, recently correct: 1.0%, eta: 03s\n",
      "Checked 18900 (37.8%) trees, correct: 3328, recently correct: 8.0%, eta: 03s\n",
      "Checked 19000 (38.0%) trees, correct: 3330, recently correct: 2.0%, eta: 03s\n",
      "Checked 19100 (38.2%) trees, correct: 3330, recently correct: 0.0%, eta: 03s\n",
      "Checked 19200 (38.4%) trees, correct: 3334, recently correct: 4.0%, eta: 03s\n",
      "Checked 19300 (38.6%) trees, correct: 3336, recently correct: 2.0%, eta: 03s\n",
      "Checked 19400 (38.8%) trees, correct: 3340, recently correct: 4.0%, eta: 03s\n",
      "Checked 19500 (39.0%) trees, correct: 3343, recently correct: 3.0%, eta: 03s\n",
      "Checked 19600 (39.2%) trees, correct: 3346, recently correct: 3.0%, eta: 03s\n",
      "Checked 19700 (39.4%) trees, correct: 3347, recently correct: 1.0%, eta: 03s\n",
      "Checked 19800 (39.6%) trees, correct: 3352, recently correct: 5.0%, eta: 03s\n",
      "Checked 19900 (39.8%) trees, correct: 3352, recently correct: 0.0%, eta: 03s\n",
      "Checked 20000 (40.0%) trees, correct: 3352, recently correct: 0.0%, eta: 03s\n",
      "Checked 20100 (40.2%) trees, correct: 3353, recently correct: 1.0%, eta: 03s\n",
      "Checked 20200 (40.4%) trees, correct: 3353, recently correct: 0.0%, eta: 03s\n",
      "Checked 20300 (40.6%) trees, correct: 3355, recently correct: 2.0%, eta: 03s\n",
      "Checked 20400 (40.8%) trees, correct: 3357, recently correct: 2.0%, eta: 03s\n",
      "Checked 20500 (41.0%) trees, correct: 3360, recently correct: 3.0%, eta: 03s\n",
      "Checked 20600 (41.2%) trees, correct: 3364, recently correct: 4.0%, eta: 03s\n",
      "Checked 20700 (41.4%) trees, correct: 3364, recently correct: 0.0%, eta: 03s\n",
      "Checked 20800 (41.6%) trees, correct: 3364, recently correct: 0.0%, eta: 03s\n",
      "Checked 20900 (41.8%) trees, correct: 3366, recently correct: 2.0%, eta: 03s\n",
      "Checked 21000 (42.0%) trees, correct: 3366, recently correct: 0.0%, eta: 03s\n",
      "Checked 21100 (42.2%) trees, correct: 3366, recently correct: 0.0%, eta: 03s\n",
      "Checked 21200 (42.4%) trees, correct: 3367, recently correct: 1.0%, eta: 03s\n",
      "Checked 21300 (42.6%) trees, correct: 3371, recently correct: 4.0%, eta: 03s\n",
      "Checked 21400 (42.8%) trees, correct: 3373, recently correct: 2.0%, eta: 03s\n",
      "Checked 21500 (43.0%) trees, correct: 3376, recently correct: 3.0%, eta: 03s\n",
      "Checked 21600 (43.2%) trees, correct: 3378, recently correct: 2.0%, eta: 03s\n",
      "Checked 21700 (43.4%) trees, correct: 3380, recently correct: 2.0%, eta: 03s\n",
      "Checked 21800 (43.6%) trees, correct: 3380, recently correct: 0.0%, eta: 03s\n",
      "Checked 21900 (43.8%) trees, correct: 3380, recently correct: 0.0%, eta: 03s\n",
      "Checked 22000 (44.0%) trees, correct: 3383, recently correct: 3.0%, eta: 03s\n",
      "Checked 22100 (44.2%) trees, correct: 3384, recently correct: 1.0%, eta: 03s\n",
      "Checked 22200 (44.4%) trees, correct: 3385, recently correct: 1.0%, eta: 03s\n",
      "Checked 22300 (44.6%) trees, correct: 3387, recently correct: 2.0%, eta: 03s\n",
      "Checked 22400 (44.8%) trees, correct: 3389, recently correct: 2.0%, eta: 03s\n",
      "Checked 22500 (45.0%) trees, correct: 3390, recently correct: 1.0%, eta: 03s\n",
      "Checked 22600 (45.2%) trees, correct: 3392, recently correct: 2.0%, eta: 03s\n",
      "Checked 22700 (45.4%) trees, correct: 3393, recently correct: 1.0%, eta: 03s\n",
      "Checked 22800 (45.6%) trees, correct: 3396, recently correct: 3.0%, eta: 03s\n",
      "Checked 22900 (45.8%) trees, correct: 3397, recently correct: 1.0%, eta: 03s\n",
      "Checked 23000 (46.0%) trees, correct: 3397, recently correct: 0.0%, eta: 03s\n",
      "Checked 23100 (46.2%) trees, correct: 3399, recently correct: 2.0%, eta: 02s\n",
      "Checked 23200 (46.4%) trees, correct: 3399, recently correct: 0.0%, eta: 02s\n",
      "Checked 23300 (46.6%) trees, correct: 3399, recently correct: 0.0%, eta: 02s\n",
      "Checked 23400 (46.8%) trees, correct: 3400, recently correct: 1.0%, eta: 02s\n",
      "Checked 23500 (47.0%) trees, correct: 3400, recently correct: 0.0%, eta: 02s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 23600 (47.2%) trees, correct: 3400, recently correct: 0.0%, eta: 02s\n",
      "Checked 23700 (47.4%) trees, correct: 3400, recently correct: 0.0%, eta: 02s\n",
      "Checked 23800 (47.6%) trees, correct: 3400, recently correct: 0.0%, eta: 02s\n",
      "Checked 23900 (47.8%) trees, correct: 3402, recently correct: 2.0%, eta: 02s\n",
      "Checked 24000 (48.0%) trees, correct: 3403, recently correct: 1.0%, eta: 02s\n",
      "Checked 24100 (48.2%) trees, correct: 3404, recently correct: 1.0%, eta: 02s\n",
      "Checked 24200 (48.4%) trees, correct: 3404, recently correct: 0.0%, eta: 02s\n",
      "Checked 24300 (48.6%) trees, correct: 3405, recently correct: 1.0%, eta: 02s\n",
      "Checked 24400 (48.8%) trees, correct: 3406, recently correct: 1.0%, eta: 02s\n",
      "Checked 24500 (49.0%) trees, correct: 3407, recently correct: 1.0%, eta: 02s\n",
      "Checked 24600 (49.2%) trees, correct: 3407, recently correct: 0.0%, eta: 02s\n",
      "Checked 24700 (49.4%) trees, correct: 3408, recently correct: 1.0%, eta: 02s\n",
      "Checked 24800 (49.6%) trees, correct: 3411, recently correct: 3.0%, eta: 02s\n",
      "Checked 24900 (49.8%) trees, correct: 3412, recently correct: 1.0%, eta: 02s\n",
      "Checked 25000 (50.0%) trees, correct: 3412, recently correct: 0.0%, eta: 02s\n",
      "Checked 25100 (50.2%) trees, correct: 3412, recently correct: 0.0%, eta: 02s\n",
      "Checked 25200 (50.4%) trees, correct: 3412, recently correct: 0.0%, eta: 02s\n",
      "Checked 25300 (50.6%) trees, correct: 3413, recently correct: 1.0%, eta: 02s\n",
      "Checked 25400 (50.8%) trees, correct: 3414, recently correct: 1.0%, eta: 02s\n",
      "Checked 25500 (51.0%) trees, correct: 3415, recently correct: 1.0%, eta: 02s\n",
      "Checked 25600 (51.2%) trees, correct: 3415, recently correct: 0.0%, eta: 02s\n",
      "Checked 25700 (51.4%) trees, correct: 3417, recently correct: 2.0%, eta: 02s\n",
      "Checked 25800 (51.6%) trees, correct: 3420, recently correct: 3.0%, eta: 02s\n",
      "Checked 25900 (51.8%) trees, correct: 3423, recently correct: 3.0%, eta: 02s\n",
      "Checked 26000 (52.0%) trees, correct: 3423, recently correct: 0.0%, eta: 02s\n",
      "Checked 26100 (52.2%) trees, correct: 3423, recently correct: 0.0%, eta: 02s\n",
      "Checked 26200 (52.4%) trees, correct: 3423, recently correct: 0.0%, eta: 02s\n",
      "Checked 26300 (52.6%) trees, correct: 3424, recently correct: 1.0%, eta: 02s\n",
      "Checked 26400 (52.8%) trees, correct: 3425, recently correct: 1.0%, eta: 02s\n",
      "Checked 26500 (53.0%) trees, correct: 3428, recently correct: 3.0%, eta: 02s\n",
      "Checked 26600 (53.2%) trees, correct: 3428, recently correct: 0.0%, eta: 02s\n",
      "Checked 26700 (53.4%) trees, correct: 3428, recently correct: 0.0%, eta: 02s\n",
      "Checked 26800 (53.6%) trees, correct: 3433, recently correct: 5.0%, eta: 02s\n",
      "Checked 26900 (53.8%) trees, correct: 3436, recently correct: 3.0%, eta: 02s\n",
      "Checked 27000 (54.0%) trees, correct: 3436, recently correct: 0.0%, eta: 02s\n",
      "Checked 27100 (54.2%) trees, correct: 3436, recently correct: 0.0%, eta: 02s\n",
      "Checked 27200 (54.4%) trees, correct: 3438, recently correct: 2.0%, eta: 02s\n",
      "Checked 27300 (54.6%) trees, correct: 3438, recently correct: 0.0%, eta: 02s\n",
      "Checked 27400 (54.8%) trees, correct: 3438, recently correct: 0.0%, eta: 02s\n",
      "Checked 27500 (55.0%) trees, correct: 3438, recently correct: 0.0%, eta: 02s\n",
      "Checked 27600 (55.2%) trees, correct: 3440, recently correct: 2.0%, eta: 02s\n",
      "Checked 27700 (55.4%) trees, correct: 3440, recently correct: 0.0%, eta: 02s\n",
      "Checked 27800 (55.6%) trees, correct: 3440, recently correct: 0.0%, eta: 02s\n",
      "Checked 27900 (55.8%) trees, correct: 3442, recently correct: 2.0%, eta: 02s\n",
      "Checked 28000 (56.0%) trees, correct: 3442, recently correct: 0.0%, eta: 02s\n",
      "Checked 28100 (56.2%) trees, correct: 3442, recently correct: 0.0%, eta: 02s\n",
      "Checked 28200 (56.4%) trees, correct: 3442, recently correct: 0.0%, eta: 02s\n",
      "Checked 28300 (56.6%) trees, correct: 3443, recently correct: 1.0%, eta: 02s\n",
      "Checked 28400 (56.8%) trees, correct: 3443, recently correct: 0.0%, eta: 02s\n",
      "Checked 28500 (57.0%) trees, correct: 3444, recently correct: 1.0%, eta: 02s\n",
      "Checked 28600 (57.2%) trees, correct: 3444, recently correct: 0.0%, eta: 02s\n",
      "Checked 28700 (57.4%) trees, correct: 3446, recently correct: 2.0%, eta: 02s\n",
      "Checked 28800 (57.6%) trees, correct: 3446, recently correct: 0.0%, eta: 02s\n",
      "Checked 28900 (57.8%) trees, correct: 3447, recently correct: 1.0%, eta: 02s\n",
      "Checked 29000 (58.0%) trees, correct: 3447, recently correct: 0.0%, eta: 02s\n",
      "Checked 29100 (58.2%) trees, correct: 3447, recently correct: 0.0%, eta: 02s\n",
      "Checked 29200 (58.4%) trees, correct: 3447, recently correct: 0.0%, eta: 02s\n",
      "Checked 29300 (58.6%) trees, correct: 3448, recently correct: 1.0%, eta: 02s\n",
      "Checked 29400 (58.8%) trees, correct: 3448, recently correct: 0.0%, eta: 02s\n",
      "Checked 29500 (59.0%) trees, correct: 3449, recently correct: 1.0%, eta: 02s\n",
      "Checked 29600 (59.2%) trees, correct: 3449, recently correct: 0.0%, eta: 02s\n",
      "Checked 29700 (59.4%) trees, correct: 3449, recently correct: 0.0%, eta: 02s\n",
      "Checked 29800 (59.6%) trees, correct: 3450, recently correct: 1.0%, eta: 02s\n",
      "Checked 29900 (59.8%) trees, correct: 3450, recently correct: 0.0%, eta: 02s\n",
      "Checked 30000 (60.0%) trees, correct: 3451, recently correct: 1.0%, eta: 02s\n",
      "Checked 30100 (60.2%) trees, correct: 3452, recently correct: 1.0%, eta: 02s\n",
      "Checked 30200 (60.4%) trees, correct: 3452, recently correct: 0.0%, eta: 02s\n",
      "Checked 30300 (60.6%) trees, correct: 3453, recently correct: 1.0%, eta: 02s\n",
      "Checked 30400 (60.8%) trees, correct: 3453, recently correct: 0.0%, eta: 02s\n",
      "Checked 30500 (61.0%) trees, correct: 3455, recently correct: 2.0%, eta: 02s\n",
      "Checked 30600 (61.2%) trees, correct: 3456, recently correct: 1.0%, eta: 02s\n",
      "Checked 30700 (61.4%) trees, correct: 3456, recently correct: 0.0%, eta: 02s\n",
      "Checked 30800 (61.6%) trees, correct: 3456, recently correct: 0.0%, eta: 02s\n",
      "Checked 30900 (61.8%) trees, correct: 3457, recently correct: 1.0%, eta: 02s\n",
      "Checked 31000 (62.0%) trees, correct: 3459, recently correct: 2.0%, eta: 02s\n",
      "Checked 31100 (62.2%) trees, correct: 3459, recently correct: 0.0%, eta: 02s\n",
      "Checked 31200 (62.4%) trees, correct: 3459, recently correct: 0.0%, eta: 02s\n",
      "Checked 31300 (62.6%) trees, correct: 3459, recently correct: 0.0%, eta: 02s\n",
      "Checked 31400 (62.8%) trees, correct: 3461, recently correct: 2.0%, eta: 02s\n",
      "Checked 31500 (63.0%) trees, correct: 3461, recently correct: 0.0%, eta: 02s\n",
      "Checked 31600 (63.2%) trees, correct: 3461, recently correct: 0.0%, eta: 02s\n",
      "Checked 31700 (63.4%) trees, correct: 3461, recently correct: 0.0%, eta: 02s\n",
      "Checked 31800 (63.6%) trees, correct: 3461, recently correct: 0.0%, eta: 02s\n",
      "Checked 31900 (63.8%) trees, correct: 3461, recently correct: 0.0%, eta: 02s\n",
      "Checked 32000 (64.0%) trees, correct: 3461, recently correct: 0.0%, eta: 02s\n",
      "Checked 32100 (64.2%) trees, correct: 3461, recently correct: 0.0%, eta: 02s\n",
      "Checked 32200 (64.4%) trees, correct: 3461, recently correct: 0.0%, eta: 02s\n",
      "Checked 32300 (64.6%) trees, correct: 3461, recently correct: 0.0%, eta: 02s\n",
      "Checked 32400 (64.8%) trees, correct: 3462, recently correct: 1.0%, eta: 02s\n",
      "Checked 32500 (65.0%) trees, correct: 3462, recently correct: 0.0%, eta: 02s\n",
      "Checked 32600 (65.2%) trees, correct: 3462, recently correct: 0.0%, eta: 02s\n",
      "Checked 32700 (65.4%) trees, correct: 3462, recently correct: 0.0%, eta: 02s\n",
      "Checked 32800 (65.6%) trees, correct: 3462, recently correct: 0.0%, eta: 02s\n",
      "Checked 32900 (65.8%) trees, correct: 3462, recently correct: 0.0%, eta: 01s\n",
      "Checked 33000 (66.0%) trees, correct: 3463, recently correct: 1.0%, eta: 01s\n",
      "Checked 33100 (66.2%) trees, correct: 3464, recently correct: 1.0%, eta: 01s\n",
      "Checked 33200 (66.4%) trees, correct: 3465, recently correct: 1.0%, eta: 01s\n",
      "Checked 33300 (66.6%) trees, correct: 3465, recently correct: 0.0%, eta: 01s\n",
      "Checked 33400 (66.8%) trees, correct: 3465, recently correct: 0.0%, eta: 01s\n",
      "Checked 33500 (67.0%) trees, correct: 3465, recently correct: 0.0%, eta: 01s\n",
      "Checked 33600 (67.2%) trees, correct: 3465, recently correct: 0.0%, eta: 01s\n",
      "Checked 33700 (67.4%) trees, correct: 3465, recently correct: 0.0%, eta: 01s\n",
      "Checked 33800 (67.6%) trees, correct: 3465, recently correct: 0.0%, eta: 01s\n",
      "Checked 33900 (67.8%) trees, correct: 3466, recently correct: 1.0%, eta: 01s\n",
      "Checked 34000 (68.0%) trees, correct: 3467, recently correct: 1.0%, eta: 01s\n",
      "Checked 34100 (68.2%) trees, correct: 3467, recently correct: 0.0%, eta: 01s\n",
      "Checked 34200 (68.4%) trees, correct: 3467, recently correct: 0.0%, eta: 01s\n",
      "Checked 34300 (68.6%) trees, correct: 3467, recently correct: 0.0%, eta: 01s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 34400 (68.8%) trees, correct: 3467, recently correct: 0.0%, eta: 01s\n",
      "Checked 34500 (69.0%) trees, correct: 3468, recently correct: 1.0%, eta: 01s\n",
      "Checked 34600 (69.2%) trees, correct: 3469, recently correct: 1.0%, eta: 01s\n",
      "Checked 34700 (69.4%) trees, correct: 3469, recently correct: 0.0%, eta: 01s\n",
      "Checked 34800 (69.6%) trees, correct: 3469, recently correct: 0.0%, eta: 01s\n",
      "Checked 34900 (69.8%) trees, correct: 3469, recently correct: 0.0%, eta: 01s\n",
      "Checked 35000 (70.0%) trees, correct: 3469, recently correct: 0.0%, eta: 01s\n",
      "Checked 35100 (70.2%) trees, correct: 3469, recently correct: 0.0%, eta: 01s\n",
      "Checked 35200 (70.4%) trees, correct: 3469, recently correct: 0.0%, eta: 01s\n",
      "Checked 35300 (70.6%) trees, correct: 3470, recently correct: 1.0%, eta: 01s\n",
      "Checked 35400 (70.8%) trees, correct: 3470, recently correct: 0.0%, eta: 01s\n",
      "Checked 35500 (71.0%) trees, correct: 3471, recently correct: 1.0%, eta: 01s\n",
      "Checked 35600 (71.2%) trees, correct: 3471, recently correct: 0.0%, eta: 01s\n",
      "Checked 35700 (71.4%) trees, correct: 3471, recently correct: 0.0%, eta: 01s\n",
      "Checked 35800 (71.6%) trees, correct: 3472, recently correct: 1.0%, eta: 01s\n",
      "Checked 35900 (71.8%) trees, correct: 3473, recently correct: 1.0%, eta: 01s\n",
      "Checked 36000 (72.0%) trees, correct: 3473, recently correct: 0.0%, eta: 01s\n",
      "Checked 36100 (72.2%) trees, correct: 3474, recently correct: 1.0%, eta: 01s\n",
      "Checked 36200 (72.4%) trees, correct: 3474, recently correct: 0.0%, eta: 01s\n",
      "Checked 36300 (72.6%) trees, correct: 3474, recently correct: 0.0%, eta: 01s\n",
      "Checked 36400 (72.8%) trees, correct: 3474, recently correct: 0.0%, eta: 01s\n",
      "Checked 36500 (73.0%) trees, correct: 3474, recently correct: 0.0%, eta: 01s\n",
      "Checked 36600 (73.2%) trees, correct: 3474, recently correct: 0.0%, eta: 01s\n",
      "Checked 36700 (73.4%) trees, correct: 3476, recently correct: 2.0%, eta: 01s\n",
      "Checked 36800 (73.6%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 36900 (73.8%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 37000 (74.0%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 37100 (74.2%) trees, correct: 3476, recently correct: 0.0%, eta: 01s\n",
      "Checked 37200 (74.4%) trees, correct: 3477, recently correct: 1.0%, eta: 01s\n",
      "Checked 37300 (74.6%) trees, correct: 3477, recently correct: 0.0%, eta: 01s\n",
      "Checked 37400 (74.8%) trees, correct: 3477, recently correct: 0.0%, eta: 01s\n",
      "Checked 37500 (75.0%) trees, correct: 3477, recently correct: 0.0%, eta: 01s\n",
      "Checked 37600 (75.2%) trees, correct: 3477, recently correct: 0.0%, eta: 01s\n",
      "Checked 37700 (75.4%) trees, correct: 3477, recently correct: 0.0%, eta: 01s\n",
      "Checked 37800 (75.6%) trees, correct: 3478, recently correct: 1.0%, eta: 01s\n",
      "Checked 37900 (75.8%) trees, correct: 3478, recently correct: 0.0%, eta: 01s\n",
      "Checked 38000 (76.0%) trees, correct: 3480, recently correct: 2.0%, eta: 01s\n",
      "Checked 38100 (76.2%) trees, correct: 3481, recently correct: 1.0%, eta: 01s\n",
      "Checked 38200 (76.4%) trees, correct: 3481, recently correct: 0.0%, eta: 01s\n",
      "Checked 38300 (76.6%) trees, correct: 3481, recently correct: 0.0%, eta: 01s\n",
      "Checked 38400 (76.8%) trees, correct: 3481, recently correct: 0.0%, eta: 01s\n",
      "Checked 38500 (77.0%) trees, correct: 3481, recently correct: 0.0%, eta: 01s\n",
      "Checked 38600 (77.2%) trees, correct: 3481, recently correct: 0.0%, eta: 01s\n",
      "Checked 38700 (77.4%) trees, correct: 3481, recently correct: 0.0%, eta: 01s\n",
      "Checked 38800 (77.6%) trees, correct: 3481, recently correct: 0.0%, eta: 01s\n",
      "Checked 38900 (77.8%) trees, correct: 3481, recently correct: 0.0%, eta: 01s\n",
      "Checked 39000 (78.0%) trees, correct: 3481, recently correct: 0.0%, eta: 01s\n",
      "Checked 39100 (78.2%) trees, correct: 3482, recently correct: 1.0%, eta: 01s\n",
      "Checked 39200 (78.4%) trees, correct: 3482, recently correct: 0.0%, eta: 01s\n",
      "Checked 39300 (78.6%) trees, correct: 3482, recently correct: 0.0%, eta: 01s\n",
      "Checked 39400 (78.8%) trees, correct: 3482, recently correct: 0.0%, eta: 01s\n",
      "Checked 39500 (79.0%) trees, correct: 3483, recently correct: 1.0%, eta: 01s\n",
      "Checked 39600 (79.2%) trees, correct: 3483, recently correct: 0.0%, eta: 01s\n",
      "Checked 39700 (79.4%) trees, correct: 3483, recently correct: 0.0%, eta: 01s\n",
      "Checked 39800 (79.6%) trees, correct: 3483, recently correct: 0.0%, eta: 01s\n",
      "Checked 39900 (79.8%) trees, correct: 3483, recently correct: 0.0%, eta: 01s\n",
      "Checked 40000 (80.0%) trees, correct: 3483, recently correct: 0.0%, eta: 01s\n",
      "Checked 40100 (80.2%) trees, correct: 3483, recently correct: 0.0%, eta: 01s\n",
      "Checked 40200 (80.4%) trees, correct: 3483, recently correct: 0.0%, eta: 01s\n",
      "Checked 40300 (80.6%) trees, correct: 3483, recently correct: 0.0%, eta: 01s\n",
      "Checked 40400 (80.8%) trees, correct: 3483, recently correct: 0.0%, eta: 01s\n",
      "Checked 40500 (81.0%) trees, correct: 3483, recently correct: 0.0%, eta: 01s\n",
      "Checked 40600 (81.2%) trees, correct: 3483, recently correct: 0.0%, eta: 01s\n",
      "Checked 40700 (81.4%) trees, correct: 3484, recently correct: 1.0%, eta: 01s\n",
      "Checked 40800 (81.6%) trees, correct: 3484, recently correct: 0.0%, eta: 01s\n",
      "Checked 40900 (81.8%) trees, correct: 3484, recently correct: 0.0%, eta: 01s\n",
      "Checked 41000 (82.0%) trees, correct: 3484, recently correct: 0.0%, eta: 01s\n",
      "Checked 41100 (82.2%) trees, correct: 3484, recently correct: 0.0%, eta: 01s\n",
      "Checked 41200 (82.4%) trees, correct: 3484, recently correct: 0.0%, eta: 01s\n",
      "Checked 41300 (82.6%) trees, correct: 3484, recently correct: 0.0%, eta: 01s\n",
      "Checked 41400 (82.8%) trees, correct: 3484, recently correct: 0.0%, eta: 01s\n",
      "Checked 41500 (83.0%) trees, correct: 3484, recently correct: 0.0%, eta: 01s\n",
      "Checked 41600 (83.2%) trees, correct: 3484, recently correct: 0.0%, eta: 01s\n",
      "Checked 41700 (83.4%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 41800 (83.6%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 41900 (83.8%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 42000 (84.0%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 42100 (84.2%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 42200 (84.4%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 42300 (84.6%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 42400 (84.8%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 42500 (85.0%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 42600 (85.2%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 42700 (85.4%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 42800 (85.6%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 42900 (85.8%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 43000 (86.0%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 43100 (86.2%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 43200 (86.4%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 43300 (86.6%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 43400 (86.8%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 43500 (87.0%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 43600 (87.2%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 43700 (87.4%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 43800 (87.6%) trees, correct: 3484, recently correct: 0.0%, eta: 00s\n",
      "Checked 43900 (87.8%) trees, correct: 3485, recently correct: 1.0%, eta: 00s\n",
      "Checked 44000 (88.0%) trees, correct: 3485, recently correct: 0.0%, eta: 00s\n",
      "Checked 44100 (88.2%) trees, correct: 3485, recently correct: 0.0%, eta: 00s\n",
      "Checked 44200 (88.4%) trees, correct: 3485, recently correct: 0.0%, eta: 00s\n",
      "Checked 44300 (88.6%) trees, correct: 3486, recently correct: 1.0%, eta: 00s\n",
      "Checked 44400 (88.8%) trees, correct: 3486, recently correct: 0.0%, eta: 00s\n",
      "Checked 44500 (89.0%) trees, correct: 3486, recently correct: 0.0%, eta: 00s\n",
      "Checked 44600 (89.2%) trees, correct: 3486, recently correct: 0.0%, eta: 00s\n",
      "Checked 44700 (89.4%) trees, correct: 3486, recently correct: 0.0%, eta: 00s\n",
      "Checked 44800 (89.6%) trees, correct: 3486, recently correct: 0.0%, eta: 00s\n",
      "Checked 44900 (89.8%) trees, correct: 3486, recently correct: 0.0%, eta: 00s\n",
      "Checked 45000 (90.0%) trees, correct: 3486, recently correct: 0.0%, eta: 00s\n",
      "Checked 45100 (90.2%) trees, correct: 3486, recently correct: 0.0%, eta: 00s\n",
      "Checked 45200 (90.4%) trees, correct: 3486, recently correct: 0.0%, eta: 00s\n",
      "Checked 45300 (90.6%) trees, correct: 3486, recently correct: 0.0%, eta: 00s\n",
      "Checked 45400 (90.8%) trees, correct: 3486, recently correct: 0.0%, eta: 00s\n",
      "Checked 45500 (91.0%) trees, correct: 3486, recently correct: 0.0%, eta: 00s\n",
      "Checked 45600 (91.2%) trees, correct: 3486, recently correct: 0.0%, eta: 00s\n",
      "Checked 45700 (91.4%) trees, correct: 3486, recently correct: 0.0%, eta: 00s\n",
      "Checked 45800 (91.6%) trees, correct: 3486, recently correct: 0.0%, eta: 00s\n",
      "Checked 45900 (91.8%) trees, correct: 3486, recently correct: 0.0%, eta: 00s\n",
      "Checked 46000 (92.0%) trees, correct: 3487, recently correct: 1.0%, eta: 00s\n",
      "Checked 46100 (92.2%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 46200 (92.4%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 46300 (92.6%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 46400 (92.8%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 46500 (93.0%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 46600 (93.2%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 46700 (93.4%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 46800 (93.6%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 46900 (93.8%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 47000 (94.0%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 47100 (94.2%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 47200 (94.4%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 47300 (94.6%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 47400 (94.8%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 47500 (95.0%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 47600 (95.2%) trees, correct: 3487, recently correct: 0.0%, eta: 00s\n",
      "Checked 47700 (95.4%) trees, correct: 3488, recently correct: 1.0%, eta: 00s\n",
      "Checked 47800 (95.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 47900 (95.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48000 (96.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48100 (96.2%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48200 (96.4%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48300 (96.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48400 (96.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48500 (97.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48600 (97.2%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48700 (97.4%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48800 (97.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 48900 (97.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49000 (98.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49100 (98.2%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49200 (98.4%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49300 (98.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49400 (98.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49500 (99.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49600 (99.2%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49700 (99.4%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49800 (99.6%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 49900 (99.8%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "Checked 50000 (100.0%) trees, correct: 3488, recently correct: 0.0%, eta: 00s\n",
      "found trees: 3488\n",
      "infix tree: ( 5 and 5 ) sep not 3\n",
      "prefix tree: sep and 5 5 not 3\n",
      "conclusion: nnFnT\n",
      "\n",
      "infix tree: 2 sep ( 2 or 1 )\n",
      "prefix tree: sep 2 or 2 1\n",
      "conclusion: nTnnn\n",
      "\n",
      "infix tree: ( 4 or 5 ) sep ( 1 and 2 )\n",
      "prefix tree: sep or 4 5 and 1 2\n",
      "conclusion: TTnnT,TTnTn\n",
      "\n",
      "infix tree: not 4 sep 2\n",
      "prefix tree: sep not 4 2\n",
      "conclusion: nTnFn\n",
      "\n",
      "infix tree: not 5 sep ( 1 and 4 )\n",
      "prefix tree: sep not 5 and 1 4\n",
      "conclusion: TnnTF\n",
      "\n",
      "infix tree: not 4 sep ( 3 or 2 )\n",
      "prefix tree: sep not 4 or 3 2\n",
      "conclusion: nnTFn,nTnFn\n",
      "\n",
      "infix tree: ( 1 or 3 ) sep not 1\n",
      "prefix tree: sep or 1 3 not 1\n",
      "conclusion: FnTnn\n",
      "\n",
      "infix tree: ( 3 or 3 ) sep ( 4 and 4 )\n",
      "prefix tree: sep or 3 3 and 4 4\n",
      "conclusion: nnTTn\n",
      "\n",
      "infix tree: not 5 sep ( 3 or 5 )\n",
      "prefix tree: sep not 5 or 3 5\n",
      "conclusion: nnTnF\n",
      "\n",
      "infix tree: ( 4 or 2 ) sep not 5\n",
      "prefix tree: sep or 4 2 not 5\n",
      "conclusion: nnnTF,nTnnF\n",
      "\n",
      "infix tree: not 5 sep ( 2 or 3 )\n",
      "prefix tree: sep not 5 or 2 3\n",
      "conclusion: nnTnF,nTnnF\n",
      "\n",
      "infix tree: ( 2 and 5 ) sep ( 5 or 3 )\n",
      "prefix tree: sep and 2 5 or 5 3\n",
      "conclusion: nTnnT\n",
      "\n",
      "infix tree: not 2 sep 3\n",
      "prefix tree: sep not 2 3\n",
      "conclusion: nFTnn\n",
      "\n",
      "infix tree: ( 5 and 4 ) sep ( 2 and 3 )\n",
      "prefix tree: sep and 5 4 and 2 3\n",
      "conclusion: nTTTT\n",
      "\n",
      "infix tree: 3 sep ( 3 and 2 )\n",
      "prefix tree: sep 3 and 3 2\n",
      "conclusion: nTTnn\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]])]\n",
      "[array([ 0.,  0., -1.,  0.,  1.])]\n",
      "\n",
      "[array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])]\n",
      "[array([0., 1., 0., 0., 0.])]\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])]\n",
      "[array([1., 1., 0., 0., 1.]), array([1., 1., 0., 1., 0.])]\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])]\n",
      "[array([ 0.,  1.,  0., -1.,  0.])]\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])]\n",
      "[array([ 1.,  0.,  0.,  1., -1.])]\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])]\n",
      "[array([ 0.,  0.,  1., -1.,  0.]), array([ 0.,  1.,  0., -1.,  0.])]\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])]\n",
      "[array([-1.,  0.,  1.,  0.,  0.])]\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])]\n",
      "[array([0., 0., 1., 1., 0.])]\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])]\n",
      "[array([ 0.,  0.,  1.,  0., -1.])]\n",
      "\n",
      "[array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]), array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])]\n",
      "[array([ 0.,  0.,  0.,  1., -1.]), array([ 0.,  1.,  0.,  0., -1.])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "from dataset.conclusion_generation import test_for_mental_models, generate_random_tree, generate_and_save_trees\n",
    "from dataset.logic_tree import OperatorNode\n",
    "from dataset.encoding import encode_mental_models_separated_sentences, load_sentences_and_conclusions\n",
    "\n",
    "max_depth = 2\n",
    "num_variables = 5\n",
    "input_length = 10\n",
    "output_length = 5\n",
    "\n",
    "class SeparatorNode(OperatorNode):\n",
    "    accepts_children = 2\n",
    "\n",
    "    def __init__(self, *children):\n",
    "        super(SeparatorNode, self).__init__('sep', *children)\n",
    "\n",
    "    def evaluate(self, values):\n",
    "        value = self._children[0].evaluate(values)\n",
    "        for child in self._children[1:]:\n",
    "            value = value and child.evaluate(values)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def to_string(self):\n",
    "        string = f'{self._children[0].to_string()}'\n",
    "        for child in self._children[1:]:\n",
    "            string += f' {self._operator_symbol} {child.to_string()}'\n",
    "        return string\n",
    "\n",
    "\n",
    "test_for_one_mental_models = functools.partial(test_for_mental_models, allow_only_one_mental_model=False)\n",
    "# test_for_mental_models_type_two = functools.partial(test_for_mental_models, type_one=False)\n",
    "# test_for_one_mental_models_type_two = functools.partial(test_for_mental_models, type_one=False,\n",
    "#                                                         allow_only_one_mental_model=True)\n",
    "generate_random_sep_tree = functools.partial(generate_random_tree, root_node_cls=SeparatorNode)\n",
    "\n",
    "generate_and_save_trees('./data', 50000, 2, 5,\n",
    "                        test_for_one_mental_models, generate_random_sep_tree,\n",
    "                        base_name='and_trees_single_mms_type_I')\n",
    "\n",
    "encode_mental_models_separated_sentences('./data', 2, 5, 10,\n",
    "                                         'encoded_and_trees_single_mms_type_I',\n",
    "                                         'and_trees_single_mms_type_I')\n",
    "data = load_sentences_and_conclusions('./data', num_variables=5, max_depth=2,\n",
    "                                      base_name='encoded_and_trees_single_mms_type_I')\n",
    "sentences, mental_models, input_dictionary, output_dictionary = data\n",
    "\n",
    "for i in range(10):\n",
    "    print(sentences[i])\n",
    "    print(mental_models[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 7  5 10  5  6]\n",
      "  [ 8  3  0  0  0]]\n",
      "\n",
      " [[ 2  0  0  0  0]\n",
      "  [ 7  2  9  1  6]]\n",
      "\n",
      " [[ 7  4  9  5  6]\n",
      "  [ 7  1 10  2  6]]\n",
      "\n",
      " [[ 8  4  0  0  0]\n",
      "  [ 2  0  0  0  0]]\n",
      "\n",
      " [[ 8  5  0  0  0]\n",
      "  [ 7  1 10  4  6]]]\n",
      "[[[ 0  0 -1  0  1]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]]\n",
      "\n",
      " [[ 0  1  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]]\n",
      "\n",
      " [[ 1  1  0  0  1]\n",
      "  [ 1  1  0  1  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]]\n",
      "\n",
      " [[ 0  1  0 -1  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]]\n",
      "\n",
      " [[ 1  0  0  1 -1]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]\n",
      "  [ 0  0  0  0  0]]]\n",
      "max_input_length 5\n",
      "input (None, 2, 5)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2, 5)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 2, 5, 10)     110         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 2, 50)        0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, None, 5)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 2, 128), (No 91648       reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 128),  68608       input_3[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 5)      645         lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 161,011\n",
      "Trainable params: 161,011\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1000\n",
      "354/354 [==============================] - 5s 7ms/step - loss: 0.0727 - val_loss: 0.0427\n",
      "Epoch 2/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0324 - val_loss: 0.0271\n",
      "Epoch 3/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0226 - val_loss: 0.0219\n",
      "Epoch 4/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0174 - val_loss: 0.0176\n",
      "Epoch 5/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0137 - val_loss: 0.0148\n",
      "Epoch 6/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0112 - val_loss: 0.0123\n",
      "Epoch 7/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0092 - val_loss: 0.0103\n",
      "Epoch 8/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0076 - val_loss: 0.0091\n",
      "Epoch 9/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0061 - val_loss: 0.0070\n",
      "Epoch 10/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0051 - val_loss: 0.0061\n",
      "Epoch 11/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0042 - val_loss: 0.0066\n",
      "Epoch 12/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0035 - val_loss: 0.0047\n",
      "Epoch 13/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0033 - val_loss: 0.0039\n",
      "Epoch 14/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0025 - val_loss: 0.0034\n",
      "Epoch 15/1000\n",
      "354/354 [==============================] - ETA: 0s - loss: 0.002 - 2s 5ms/step - loss: 0.0024 - val_loss: 0.0034\n",
      "Epoch 16/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0020 - val_loss: 0.0027\n",
      "Epoch 17/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0019 - val_loss: 0.0027\n",
      "Epoch 18/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 19/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0024\n",
      "Epoch 20/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 21/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 22/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 23/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0010 - val_loss: 0.0017\n",
      "Epoch 24/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 9.6526e-04 - val_loss: 0.0015\n",
      "Epoch 25/1000\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 0.0010 - val_loss: 0.0018\n",
      "Epoch 26/1000\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 8.5425e-04 - val_loss: 0.0023\n",
      "Epoch 27/1000\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 9.9324e-04 - val_loss: 0.0017\n",
      "Epoch 28/1000\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 7.3899e-04 - val_loss: 0.0013\n",
      "Epoch 29/1000\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 8.2326e-04 - val_loss: 0.0014\n",
      "Epoch 30/1000\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 6.5651e-04 - val_loss: 0.0013\n",
      "Epoch 31/1000\n",
      "354/354 [==============================] - 2s 7ms/step - loss: 5.8094e-04 - val_loss: 0.0011\n",
      "Epoch 32/1000\n",
      "354/354 [==============================] - 2s 7ms/step - loss: 5.9460e-04 - val_loss: 0.0012\n",
      "Epoch 33/1000\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 6.2776e-04 - val_loss: 0.0011\n",
      "Epoch 34/1000\n",
      "354/354 [==============================] - 2s 7ms/step - loss: 5.1764e-04 - val_loss: 0.0011\n",
      "Epoch 35/1000\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 5.5167e-04 - val_loss: 0.0011\n",
      "Epoch 36/1000\n",
      "354/354 [==============================] - 2s 7ms/step - loss: 5.6173e-04 - val_loss: 0.0011\n",
      "Epoch 37/1000\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 6.1183e-04 - val_loss: 0.0012\n",
      "Epoch 38/1000\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 9.6951e-04 - val_loss: 0.0011\n",
      "Epoch 39/1000\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 3.9869e-04 - val_loss: 9.1010e-04\n",
      "Epoch 40/1000\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 3.2915e-04 - val_loss: 8.1228e-04\n",
      "Epoch 41/1000\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 3.1537e-04 - val_loss: 7.4330e-04\n",
      "Epoch 42/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.4496e-04 - val_loss: 8.2379e-04\n",
      "Epoch 43/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.2774e-04 - val_loss: 9.7896e-04\n",
      "Epoch 44/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.5756e-04 - val_loss: 8.2310e-04\n",
      "Epoch 45/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 4.6924e-04 - val_loss: 0.0011\n",
      "Epoch 46/1000\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 3.3685e-04 - val_loss: 8.9498e-04\n",
      "Epoch 47/1000\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 2.5889e-04 - val_loss: 5.8822e-04\n",
      "Epoch 48/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 2.5009e-04 - val_loss: 7.4166e-04\n",
      "Epoch 49/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.3884e-04 - val_loss: 9.1141e-04\n",
      "Epoch 50/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 5.7515e-04 - val_loss: 8.8516e-04\n",
      "Epoch 51/1000\n",
      "354/354 [==============================] - 2s 6ms/step - loss: 2.7737e-04 - val_loss: 8.2190e-04\n",
      "Epoch 52/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354/354 [==============================] - 2s 5ms/step - loss: 2.2275e-04 - val_loss: 6.7550e-04\n",
      "Epoch 53/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.5970e-04 - val_loss: 6.5425e-04\n",
      "Epoch 54/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.7106e-04 - val_loss: 6.2788e-04\n",
      "Epoch 55/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.6921e-04 - val_loss: 6.3843e-04\n",
      "Epoch 56/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.6060e-04 - val_loss: 5.9912e-04\n",
      "Epoch 57/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.5868e-04 - val_loss: 7.6111e-04\n",
      "Epoch 58/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.7024e-04 - val_loss: 5.0190e-04\n",
      "Epoch 59/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.6670e-04 - val_loss: 6.3123e-04\n",
      "Epoch 60/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 61/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 2.2647e-04 - val_loss: 6.9112e-04\n",
      "Epoch 62/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.1977e-04 - val_loss: 7.5698e-04\n",
      "Epoch 63/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 9.9341e-05 - val_loss: 6.5113e-04\n",
      "Epoch 64/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 7.9878e-05 - val_loss: 6.6699e-04\n",
      "Epoch 65/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 8.9580e-05 - val_loss: 6.3091e-04\n",
      "Epoch 66/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.0124e-04 - val_loss: 6.6274e-04\n",
      "Epoch 67/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 2.3291e-04 - val_loss: 8.1559e-04\n",
      "Epoch 68/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.1843e-04 - val_loss: 6.1347e-04\n",
      "Epoch 69/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.2452e-04 - val_loss: 5.8753e-04\n",
      "Epoch 70/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 8.4631e-05 - val_loss: 6.1111e-04\n",
      "Epoch 71/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 6.0101e-05 - val_loss: 4.7688e-04\n",
      "Epoch 72/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 7.3903e-05 - val_loss: 5.1189e-04\n",
      "Epoch 73/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 6.9425e-05 - val_loss: 5.6283e-04\n",
      "Epoch 74/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 8.4144e-05 - val_loss: 7.6372e-04\n",
      "Epoch 75/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 7.8967e-04 - val_loss: 0.0014\n",
      "Epoch 76/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 2.7980e-04 - val_loss: 3.4084e-04\n",
      "Epoch 77/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 8.7681e-05 - val_loss: 2.1322e-04\n",
      "Epoch 78/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 5.1736e-05 - val_loss: 2.6858e-04\n",
      "Epoch 79/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 4.7796e-05 - val_loss: 2.9856e-04\n",
      "Epoch 80/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 5.3389e-05 - val_loss: 2.6755e-04\n",
      "Epoch 81/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 4.4039e-05 - val_loss: 2.9092e-04\n",
      "Epoch 82/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 5.0796e-05 - val_loss: 2.5884e-04\n",
      "Epoch 83/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 4.1790e-05 - val_loss: 3.5714e-04\n",
      "Epoch 84/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 8.7270e-05 - val_loss: 5.2072e-04\n",
      "Epoch 85/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 8.3855e-04 - val_loss: 5.9223e-04\n",
      "Epoch 86/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 1.0560e-04 - val_loss: 4.1029e-04\n",
      "Epoch 87/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 6.9979e-05 - val_loss: 4.4736e-04\n",
      "Epoch 88/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.6484e-05 - val_loss: 4.3654e-04\n",
      "Epoch 89/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.7827e-05 - val_loss: 4.3327e-04\n",
      "Epoch 90/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.0025e-05 - val_loss: 4.1910e-04\n",
      "Epoch 91/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 2.9534e-05 - val_loss: 3.8946e-04\n",
      "Epoch 92/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.2618e-05 - val_loss: 3.7349e-04\n",
      "Epoch 93/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 4.7174e-05 - val_loss: 4.0044e-04\n",
      "Epoch 94/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 5.0657e-05 - val_loss: 3.8922e-04\n",
      "Epoch 95/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 3.3864e-05 - val_loss: 3.9428e-04\n",
      "Epoch 96/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 8.5180e-05 - val_loss: 7.0813e-04\n",
      "Epoch 97/1000\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 7.7841e-04 - val_loss: 6.8925e-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors:\n",
      "( 2 or 5 )\n",
      "not 5\n",
      "[[ 0.  0.  0. -0. -1.]\n",
      " [ 0. -0.  0. -0.  0.]\n",
      " [ 0.  0.  0. -0.  0.]\n",
      " [-0.  0.  0.  0.  0.]\n",
      " [-0.  0.  0.  0.  0.]] [[ 0.  1.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[ 1.0093117e-02  4.2198116e-01  7.4109025e-02 -2.8900651e-02\n",
      "  -9.7356081e-01]\n",
      " [ 3.4245639e-04 -4.4086985e-03  2.4882872e-03 -3.9569160e-04\n",
      "   6.3640289e-03]\n",
      " [ 1.2774719e-05  6.0381833e-05  1.1701745e-03 -4.5087771e-05\n",
      "   1.0986753e-03]\n",
      " [-1.9262987e-04  1.0001648e-04  1.0876094e-03  1.9261031e-05\n",
      "   8.8084047e-04]\n",
      " [-4.1477027e-04  3.6265433e-04  8.7162189e-04  2.1984568e-05\n",
      "   4.5116668e-04]]\n",
      "\n",
      "( 5 or 2 )\n",
      "not 5\n",
      "[[ 0.  0.  0. -0. -1.]\n",
      " [ 0. -0.  0. -0.  0.]\n",
      " [ 0.  0.  0. -0.  0.]\n",
      " [-0.  0.  0. -0.  0.]\n",
      " [-0.  0.  0.  0.  0.]] [[ 0.  1.  0.  0. -1.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "[[ 4.2941246e-02  4.3501145e-01  3.3967931e-02 -2.9629191e-02\n",
      "  -9.7263539e-01]\n",
      " [ 4.1997959e-04 -4.3364656e-03  2.3830985e-03 -2.5914551e-04\n",
      "   5.4553472e-03]\n",
      " [ 1.4628749e-05  2.9413961e-05  1.1473222e-03 -4.3501030e-05\n",
      "   1.1194644e-03]\n",
      " [-1.3496680e-04  9.3398266e-05  1.0764274e-03 -2.6796479e-06\n",
      "   9.6080755e-04]\n",
      " [-3.7859846e-04  3.5663228e-04  8.6443219e-04  5.1688403e-06\n",
      "   5.2530557e-04]]\n",
      "\n",
      "errors 2\n",
      "accuracy: 99.4%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as kr\n",
    "\n",
    "\n",
    "def broadcast(x, y):\n",
    "    tf.print(x.shape, y.shape)\n",
    "    x = x[..., np.newaxis]\n",
    "    y = y[..., np.newaxis]\n",
    "    x = np.transpose(x, axes=[0, 2, 1])\n",
    "    y = np.transpose(y, axes=[2, 0, 1])\n",
    "    x, y = np.broadcast_arrays(x, y)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def calculate_values(x, y):\n",
    "    s = x + y\n",
    "    sc = np.clip(s, -1, 1)\n",
    "    return sc\n",
    "\n",
    "\n",
    "def calculate_correctness(x, y):\n",
    "    diff = 1 - np.maximum(0, np.abs(x - y) - 1)\n",
    "    prod = np.prod(diff, axis=-1)\n",
    "    return prod\n",
    "\n",
    "\n",
    "def calculate_values_soft(x, y, av=10):\n",
    "    return np.tanh((x + y) * av)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def calculate_correctness_soft(x, y, ac=10):\n",
    "    diff = 1 - sigmoid((np.abs(x - y) - 1.5) * ac)\n",
    "    prod = np.prod(diff, axis=-1)\n",
    "    return prod\n",
    "\n",
    "\n",
    "def calculate_out(values, correctness):\n",
    "    result = values * correctness[..., np.newaxis]\n",
    "    reshaped = np.reshape(result, (result.shape[0] * result.shape[1], result.shape[2]))\n",
    "    return reshaped\n",
    "\n",
    "\n",
    "def combine_mental_models(mm1, mm2):\n",
    "    mm1b, mm2b = broadcast(mm1, mm2)\n",
    "    values = calculate_values(mm1b, mm2b)\n",
    "    correctness = calculate_correctness(mm1b, mm2b)\n",
    "    out = calculate_out(values, correctness)\n",
    "    return out\n",
    "\n",
    "\n",
    "def combine_mental_models_soft(mm1, mm2):\n",
    "    mm1b, mm2b = broadcast(mm1, mm2)\n",
    "    values = calculate_values_soft(mm1b, mm2b, av=10)\n",
    "    correctness = calculate_correctness_soft(mm1b, mm2b, ac=10)\n",
    "    out = calculate_out(values, correctness)\n",
    "    return out\n",
    "\n",
    "\n",
    "def test_mm_inference():\n",
    "    # (a or b)      ---> [T, n], [n, T]\n",
    "    # (a or not b)  ---> [T, n], [n, F]\n",
    "    mm1 = np.array([\n",
    "        [1, 0],\n",
    "        [0, 1],\n",
    "    ])\n",
    "    mm2 = np.array([\n",
    "        [1, 0],\n",
    "        [0, -1]\n",
    "    ])\n",
    "\n",
    "    combined_mental_models = combine_mental_models(mm1, mm2)\n",
    "    combined_mental_models_soft = combine_mental_models_soft(mm1, mm2)\n",
    "    print(combined_mental_models)\n",
    "    print(combined_mental_models_soft)\n",
    "\n",
    "\n",
    "class MMInferenceLayer(kr.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def broadcast(self, x, y):\n",
    "        x = tf.expand_dims(x, axis=-1)\n",
    "        y = tf.expand_dims(y, axis=-1)\n",
    "        x = tf.transpose(x, perm=[0, 1, 3, 2])\n",
    "        y = tf.transpose(y, perm=[0, 3, 1, 2])\n",
    "        # x = tf.broadcast_to(x, (x.shape[0], x.shape[1]))\n",
    "        # x, y =  np.broadcast_arrays(x, y)\n",
    "        return x, y\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = inputs[0]\n",
    "        y = inputs[1]\n",
    "\n",
    "        x, y = self.broadcast(x, y)\n",
    "\n",
    "        s = x + y\n",
    "        value = tf.clip_by_value(s, -1, 1)\n",
    "        # applicability = (tf.reduce_max(tf.abs(x), axis=-1) * tf.reduce_max(tf.abs(y), axis=-1))\n",
    "        # value = value * tf.expand_dims(applicability, axis=-1)\n",
    "\n",
    "        diff = 1 - tf.maximum(0., tf.abs(x - y) - 1.)\n",
    "        correctness = tf.reduce_prod(diff, axis=-1)\n",
    "        mms = value * tf.expand_dims(correctness, axis=-1)\n",
    "        reshaped_value = tf.reshape(mms, (-1, mms.shape[-3] * mms.shape[-2], mms.shape[-1]))\n",
    "        reshaped_correctness = tf.reshape(correctness, (-1, correctness.shape[-2] * correctness.shape[-1]))\n",
    "        mm = tf.reduce_sum(reshaped_value, axis=-2)\n",
    "        mm = mm / tf.reduce_sum(reshaped_correctness, axis=-1, keepdims=True)\n",
    "        # mm = tf.clip_by_value(mm, -1, 1)\n",
    "        # mm = tf.tanh(mm)\n",
    "        return mm\n",
    "\n",
    "\n",
    "def create_inference_model(num_variables, max_input_length, max_sub_mental_models):\n",
    "    embedding_size = 10\n",
    "    hidden_units = 128\n",
    "    print('max_input_length', max_input_length)\n",
    "    input = kr.Input(shape=(2, max_input_length))\n",
    "    split_layer = kr.layers.Lambda(lambda x: (x[:, 0], x[:, 1]))(input)\n",
    "\n",
    "    nn_input = kr.Input(max_input_length)\n",
    "    nn_embedding_layer = kr.layers.Embedding(num_symbols + 1, embedding_size)(nn_input);print(nn_embedding_layer)\n",
    "    flatten_layer = kr.layers.Flatten()(nn_embedding_layer);print(flatten_layer.shape)\n",
    "    nn_hidden = kr.layers.Dense(hidden_units, activation='relu')(flatten_layer)\n",
    "    nn_output = kr.layers.Dense(num_variables * max_sub_mental_models,\n",
    "                                activation='tanh',\n",
    "                                activity_regularizer=kr.regularizers.L1(0.0))(nn_hidden)\n",
    "    nn_reshape = kr.layers.Reshape((max_sub_mental_models, num_variables))(nn_output)\n",
    "    sub_sequence_nn = kr.Model(inputs=nn_input, outputs=nn_reshape, name='sub-sequence-NN')\n",
    "    sub_sequence_nn.summary()\n",
    "\n",
    "    mm = sub_sequence_nn(split_layer[0]), sub_sequence_nn(split_layer[1])\n",
    "    mm_inference_layer = MMInferenceLayer()(mm)\n",
    "\n",
    "    model = kr.Model(inputs=input, outputs=mm_inference_layer)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_varying_inference_model(num_variables, max_input_length):\n",
    "    embedding_size = 10\n",
    "    hidden_units = 128\n",
    "    print('max_input_length', max_input_length)\n",
    "    input = kr.Input(shape=(2, max_input_length))\n",
    "\n",
    "    print('input',input.shape)\n",
    "    \n",
    "    nn_input = kr.Input(shape=(num_variables))\n",
    "    nn_embedding_layer = kr.layers.Embedding(num_symbols+1, embedding_size)(input)\n",
    "    nn_flatten = tf.keras.layers.Reshape((nn_embedding_layer.shape[1],-1))(nn_embedding_layer)\n",
    "    encoder = kr.layers.LSTM(hidden_units, return_sequences=True, return_state=True, activation='relu')\n",
    "    encoder_outputs, state_h, state_c = encoder(nn_flatten)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    decoder_inputs = kr.Input(shape=(None,num_variables))\n",
    "    decoder = kr.layers.LSTM(hidden_units, return_sequences=True, return_state=True, activation='relu')\n",
    "    decoder_outputs, _, _ = decoder(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_dense = kr.layers.Dense(num_variables, activation='tanh')\n",
    "    output = decoder_dense(decoder_outputs)\n",
    "    \n",
    "#     nn_input = kr.Input(max_input_length)\n",
    "#     nn_embedding_layer = kr.layers.Embedding(num_symbols + 1, embedding_size)(nn_input)\n",
    "#     flatten_layer = kr.layers.Flatten()(nn_embedding_layer)\n",
    "#     print('flatten',flatten_layer.shape)\n",
    "#     encoder = kr.layers.LSTM(hidden_units, return_sequences=True, return_state=True, activation='relu')\n",
    "#     encoder_outputs, state_h, state_c = encoder(flatten_layer)\n",
    "\n",
    "#     decoder = kr.layers.LSTM(hidden_units, return_sequences=True, return_state=True, activation='relu')\n",
    "#     decoder_outputs, _, _ = decoder(decoder_inputs,\n",
    "#                                          initial_state=encoder_states)\n",
    "#     decoder_dense = kr.layers.Dense(num_variables, activation='tanh')\n",
    "#     decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = kr.Model(inputs=[input, decoder_inputs], outputs=output)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def two_way_mse(y_true, y_pred):\n",
    "    y_true_float = tf.cast(y_true, y_pred.dtype)\n",
    "    diff = (y_true_float - y_pred) ** 2\n",
    "    print(diff)\n",
    "    return tf.reduce_mean(diff)\n",
    "\n",
    "\n",
    "def show_subsentence_inference(model, ds, decoding_dictionary, idxs):\n",
    "    sub_model = model.layers[2]\n",
    "    for i in idxs:\n",
    "        for j in range(2):\n",
    "            x = ds.x_test[i][j]\n",
    "            pred = sub_model.predict(x[np.newaxis, ...])\n",
    "            print(dataset.encoding.decode_sentence(x, decoding_dictionary, ds.indexed_encoding))\n",
    "            print(np.rint(pred))\n",
    "            \n",
    "def add_zero_row(data, position):\n",
    "    if position == 'front':\n",
    "        temp = np.zeros((data.shape[0],data.shape[1]+1,data.shape[2]))\n",
    "        temp[:,1:,:] = data\n",
    "    elif position == 'last':\n",
    "        temp = np.zeros((data.shape[0],data.shape[1]+1,data.shape[2]))\n",
    "        temp[:,:-1,:] = data\n",
    "        \n",
    "    return temp\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # xs, ys = create_ds()\n",
    "\n",
    "    from dataset.common import get_separated_sequences_mental_models_dataset\n",
    "    import dataset.encoding\n",
    "    ds = get_separated_sequences_mental_models_dataset('./data', 'encoded_and_trees_single_mms_type_I',\n",
    "                                                       num_variables=5, max_depth=2,\n",
    "                                                       test_size=.1, valid_size=.1,\n",
    "                                                       indexed_encoding=True, pad_mental_models=True)\n",
    "\n",
    "    dec_in, dec_out = dataset.encoding.create_decoding_dictionaries(ds.input_dictionary, ds.output_dictionary)\n",
    "\n",
    "#     ds.y_train = ds.y_train[..., 0, :]\n",
    "#     ds.y_valid = ds.y_valid[..., 0, :]\n",
    "#     ds.y_test = ds.y_test[..., 0, :]\n",
    "\n",
    "    ds.y_train_d = add_zero_row(ds.y_train, 'front')\n",
    "    ds.y_train = add_zero_row(ds.y_train, 'last')\n",
    "    ds.y_valid_d = add_zero_row(ds.y_valid, 'front')\n",
    "    ds.y_valid = add_zero_row(ds.y_valid, 'last')\n",
    "    ds.y_test_d = add_zero_row(ds.y_test, 'front')\n",
    "    ds.y_test = add_zero_row(ds.y_test, 'last')\n",
    "    \n",
    "    \n",
    "#  decoder_train = \n",
    "    \n",
    "    num_variables = 5\n",
    "    num_operators = 5  # and, or, not\n",
    "    num_symbols = num_variables + num_operators\n",
    "    max_input_length = ds.x_train.shape[-1]\n",
    "\n",
    "#     model = create_inference_model(num_variables, max_input_length,3)\n",
    "    model = create_varying_inference_model(num_variables, max_input_length)\n",
    "    model.compile(optimizer=kr.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=kr.losses.mse)\n",
    "\n",
    "    callbacks = [kr.callbacks.EarlyStopping(patience=20, min_delta=1e-5, restore_best_weights=True)]\n",
    "    history = model.fit([ds.x_train, ds.y_train_d], ds.y_train, validation_data=([ds.x_valid, ds.y_valid_d], ds.y_valid),\n",
    "                        epochs=1000, batch_size=8, callbacks=callbacks)\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    plt.plot(range(len(loss)), loss, label='loss')\n",
    "    plt.plot(range(len(val_loss)), loss, label='val_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    preds = model.predict([ds.x_test, ds.y_test_d])\n",
    "    preds_int = np.rint(preds)\n",
    "#     for i in range(preds.shape[0]):\n",
    "#         print(preds_int[i], preds[i], ds.y_test[i])\n",
    "\n",
    "    print('errors:')\n",
    "    for i in range(preds_int.shape[0]):\n",
    "        if np.sum(np.abs(preds_int[i] - ds.y_test[i])) == 0:\n",
    "            continue\n",
    "        print(dataset.encoding.decode_sentence(ds.x_test[i][0], dec_in, ds.indexed_encoding))\n",
    "        print(dataset.encoding.decode_sentence(ds.x_test[i][1], dec_in, ds.indexed_encoding))\n",
    "        print(preds_int[i], ds.y_test[i])\n",
    "        print(preds[i])\n",
    "        print()\n",
    "\n",
    "    errors = np.count_nonzero(np.sum(np.abs(preds_int - ds.y_test), axis=-1))\n",
    "    print('errors', int(errors))\n",
    "    print(f'accuracy: {int((1 - int(errors)/ds.x_test.shape[0]) * 1000) / 10}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import random\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\" A GRU-based encoder. \"\"\"\n",
    "    def __init__(self, input_vocab_size, hidden_size, emb_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_vocab_size, emb_size)\n",
    "        self.gru = nn.GRU(emb_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        if hidden is not None:  # Update hidden states step by step\n",
    "            embedded = self.embedding(input).view(1, 1, -1)\n",
    "            output = embedded\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        else:   # In case we only need the last state\n",
    "            embedded = self.embedding(input).view(len(input), 1, -1)\n",
    "            output, hidden = self.gru(embedded)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        '''Initialize hidden state'''\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\" A GRU-based decoder. \"\"\"\n",
    "    def __init__(self, hidden_size, output_vocab_size, emb_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_vocab_size, emb_size)\n",
    "        self.gru = nn.GRU(emb_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Output word embedding\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        # Update decoder hidden state\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # Distribution over output vocabulary\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        '''Initialize hidden state'''\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "sentences[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-2ac01a8b932a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Train for 3 epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0msrc_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_sequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tensor_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "input_size = sentences[0][0].shape[1]\n",
    "output_size = mental_models[0][0].size\n",
    "hidden_size = 128\n",
    "report_every = 1000\n",
    "\n",
    "# Initialize encoder and decoder\n",
    "enc = Encoder(input_size, hidden_size, emb_size=8)\n",
    "dec = Decoder(hidden_size, output_size, emb_size=8)\n",
    "\n",
    "# Initialize the optimizers\n",
    "enc_optimizer = optim.Adam(enc.parameters())\n",
    "dec_optimizer = optim.Adam(dec.parameters())\n",
    "loss_function = nn.NLLLoss()\n",
    "print_loss_total = 0\n",
    "\n",
    "for epoch in range(1, 4):  # Train for 3 epochs\n",
    "    random.shuffle(tensor_dataset)\n",
    "    for i, instance in enumerate(tensor_dataset):\n",
    "        src_sequence, tgt_sequence = instance[0], instance[1]\n",
    "        _, last_enc_state = enc(src_sequence)\n",
    "        # Create BOS token - First decoder input is the start token \n",
    "        decoder_input = torch.tensor([[generator.token_to_idx['<s>']]], device=device)\n",
    "        # First decoder hidden state is the last encoder hidden state\n",
    "        decoder_hidden = last_enc_state\n",
    "        loss = 0\n",
    "\n",
    "        # For each decoder timestep\n",
    "        for i_decoder in range(len(tgt_sequence)):\n",
    "            # Feed decoder input to decoder hidden state. Also feed in encoder outputs for calculating context\n",
    "            decoder_output, decoder_hidden = dec(decoder_input, decoder_hidden)\n",
    "            loss += loss_function(decoder_output, tgt_sequence[i_decoder])\n",
    "            # When training, use the correct label at this step at input to the decoder \n",
    "            decoder_input = tgt_sequence[i_decoder]\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        enc_optimizer.step()\n",
    "        dec_optimizer.step()\n",
    "        enc.zero_grad()\n",
    "        dec.zero_grad()\n",
    "\n",
    "        print_loss_total += loss.item() / len(tgt_sequence)\n",
    "\n",
    "        if (i != 0 and i % report_every == 0) or i == len(tensor_dataset)-1:\n",
    "            print('Epoch {0}: trained {1} input-output pairs, loss {2:.4f}'.format(epoch, i, print_loss_total / report_every))\n",
    "            print_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tests = 10\n",
    "n_correct = 0\n",
    "input_min_val = 10\n",
    "input_max_val = 100\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(n_tests):\n",
    "        test_pair = generator.generate_equation(input_min_val, input_max_val)\n",
    "        input_seq = tensor_from_character_sequence(test_pair[0], generator.token_to_idx)\n",
    "\n",
    "        _, last_enc_state = enc(input_seq)\n",
    "\n",
    "        # Create BOS token - First decoder input is the start token \n",
    "        decoder_input = torch.tensor([generator.token_to_idx['<s>']], device=device)\n",
    "        # First decoder hidden state is the last encoder hidden state\n",
    "        decoder_hidden = last_enc_state\n",
    "\n",
    "        decoded_words = []\n",
    "        # For each decoder timestep\n",
    "        for i_decoder in range(10):\n",
    "            # Feed decoder input to decoder hidden state. Also feed in encoder outputs for calculating context\n",
    "            decoder_output, decoder_hidden = dec(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "\n",
    "            if topi.item() == generator.token_to_idx['</s>']:\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(generator.idx_to_token[topi.item()])\n",
    "            # When testing, use the model prediction\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        model_output = ''.join(decoded_words)\n",
    "\n",
    "        if model_output == test_pair[1]:\n",
    "            n_correct += 1\n",
    "            marker = ''\n",
    "        else:\n",
    "            marker = '<--------'\n",
    "        print('Input:\\t', test_pair[0])\n",
    "        print('Model output:\\t', model_output, marker)\n",
    "        print('Ground truth:\\t', test_pair[1], '\\n')\n",
    "\n",
    "    print('{0}/{1}, correct rate: {2:0.0%}'.format(n_correct, n_tests, n_correct/n_tests))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
